{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the difference between nucleus and TFS set of words. See if the words in this set are reasonable. They should be reasonable for when TFS is looser and should not be when TFS is tighter. Out of this set of words, take the highest, lowest prob ones and the middle one? Need at least 3 word gap between them. \n",
    "Rank each one in how reasonable it is. Control baseline is how reasonable words at different points in the body are. The first word, half way and at the closer of the tails. Can also get words outside of the last tail, just outside, halfway to the end, and at the very end. \n",
    "There are 9 different words in total. Should i rank all of them, scramble up a subset? Or have a ranking for each of the subsets differently? Probably scramble them all up. And then present 3 of them??\n",
    "Can then see overall how replaceable different parts of the distributions is, and for TFS and nucleus specifically. Have a control with the bad words. And should be able to hopefully show that the difference in replaceability between the TFS and nucleus words is small when TFS is looser and large when TFS is tighter. Can see how specific this is also. Eg could just look at the last TFS and nucleus words, see the diff in replaceability, or maybe need to look at the averages. Look most at this unique subset and these three points should hopefully categorize it. Want them because i dont know how good the signal is going to be. For the closest words for example. \n",
    "Can also get overall data on how replaceability corresponds to model probability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pandas as pd    \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decodeLogits import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" {'tfs':[0.25, 0.75, 0.9, 0.95, 0.99], 'flat':[0.01, 0.02, 0.05],\\n'n': [0.1, 0.25, 0.5, 0.63, 0.69, 0.75, 0.81, 0.9], 'k':[1,10,40,200]  }\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#updated\n",
    "vals_dict = {'tfs':[0.25, 0.75, 0.9, 0.95, 0.99],\n",
    "'n': [0.5, 0.63, 0.69, 0.81, 0.75, 0.9], 'k':[1,40,200]  }\n",
    "\n",
    "\n",
    "''' {'tfs':[0.25, 0.75, 0.9, 0.95, 0.99], 'flat':[0.01, 0.02, 0.05],\n",
    "'n': [0.1, 0.25, 0.5, 0.63, 0.69, 0.75, 0.81, 0.9], 'k':[1,10,40,200]  }'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing analysis for the same logit going to use the ground truth as it is a Schelling point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "\n",
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=25\n",
    "num_batches=4\n",
    "prompt_length=100\n",
    "generated_length=150\n",
    "tot_len = prompt_length+generated_length\n",
    "\n",
    "import encoder\n",
    "from decodeLogits import *\n",
    "model_name='774M' #345M\n",
    "models_dir='../gpt-2/models'\n",
    "enc = encoder.get_encoder(model_name, models_dir)\n",
    "\n",
    "prompts=pd.read_csv('test_dataframe_500primer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_out_path = 'gpt-2_output/'\n",
    "additional_path = '-model_774M-seed_27'#'' \n",
    "#all_perps = pickle.load( gzip.open(gpt_out_path+'all_perplexities_perplexity_scores_for_the_dataset_Human_StoryPrompts_Completion.csv'+additional_path+'.pickle.gz', 'rb'))\n",
    "all_logits = pickle.load( gzip.open(gpt_out_path+'all_logits_perplexity_scores_for_the_dataset_Human_StoryPrompts_Completion.csv'+additional_path+'.pickle.gz', 'rb')) # needed to get the probabilities\n",
    "text = pickle.load( gzip.open(gpt_out_path+'all_text_perplexity_scores_for_the_dataset_Human_StoryPrompts_Completion.csv'+additional_path+'.pickle.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "24\n",
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_num = 0\n",
    "mapping_to_batch = dict()\n",
    "for i in range(num_batches):\n",
    "    num_in_batch = all_logits[i].shape[0]\n",
    "    for el, b_ind in zip(range(tot_num, tot_num+num_in_batch), range(0, num_in_batch)):\n",
    "        mapping_to_batch[el] = (i, b_ind) # actual batch and then the ind in that batch\n",
    "    print( num_in_batch)\n",
    "    tot_num+= num_in_batch\n",
    "tot_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 150, 50257)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to generate things that are at the different probability levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at max deviation and getting these points within the tails of each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to also focus on the maximum deviation portions at some point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random prompts and locations within the 150 generation locations\n",
    "\n",
    "num_prompts_and_timepoints_wanted = 50 # cant be larger than 100 right now!!!\n",
    "assert num_prompts_and_timepoints_wanted <=100\n",
    "leading_prompt = 15\n",
    "\n",
    "want_min_max = False\n",
    "\n",
    "if want_min_max==True:\n",
    "    minmax = pd.read_csv('Min_Max_Disagreement_Coords_TFS90.csv')\n",
    "    #shuffle it and take the number that is wanted. \n",
    "    minmax = minmax.sample(frac= num_prompts_and_timepoints_wanted/minmax.shape[0])\n",
    "    display(minmax.head())\n",
    "    \n",
    "else:\n",
    "    rand_prompts = np.random.randint(0,tot_num, num_prompts_and_timepoints_wanted)\n",
    "    # leadingprompt until 150. as the logits are 150 not 250. \n",
    "    # -1 because of the end token prediction\n",
    "    rand_times = np.random.randint(0,generated_length-1, num_prompts_and_timepoints_wanted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_logits[0][0, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting random locations\n",
    "for the differnet gen strategies. find the tail locations. take the tokens at each position. \n",
    "move onto the next random location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "\n",
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder.get_encoder(model_name, models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground token 383\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 220\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 8449\n",
      "decoded ground word  Princess\n",
      "ground word prob is:  0.00081096415\n",
      "sorted indices [38539 21092 44910 38389  7011 30141 26199   569 21477   262]\n",
      "ten sps values [0.09158789 0.03608082 0.03481364 0.02311325 0.02190365 0.02131161\n",
      " 0.01902784 0.01692008 0.01686328 0.0158859 ]\n",
      "prob slices: [0.09158789 0.07327031 0.05495274 0.03663516 0.01831758 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 6, 35195]\n",
      "absolute positions of slice words [0, 1, 2, 3, 6, 50152]\n",
      "['Demons' 'Wizards' 'Witches' 'Ghosts' 'Devils' 'cumbers']\n",
      "ground token 314\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 921\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 465\n",
      "decoded ground word  his\n",
      "ground word prob is:  0.1101153\n",
      "sorted indices [  284   407   262   465   257   326   428   281 27232   220]\n",
      "ten sps values [0.3743754  0.15168922 0.114058   0.1101153  0.05774547 0.02652512\n",
      " 0.02458067 0.00658348 0.00655851 0.00533612]\n",
      "prob slices: [0.3743754  0.29950032 0.22462524 0.14975016 0.07487508 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35195]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50143]\n",
      "['to' 'not' 'the' 'his' 'a' 'abase']\n",
      "ground token 468\n",
      "decoded ground word  has\n",
      "ground word prob is:  0.8046437\n",
      "sorted indices [ 468  550  714  460  423 1683 4394 1088  318  564]\n",
      "ten sps values [0.8046437  0.11674176 0.02179473 0.01425535 0.0040541  0.0024963\n",
      " 0.00198535 0.00196201 0.00158396 0.00122511]\n",
      "prob slices: [0.80464369 0.64371495 0.48278621 0.32185748 0.16092874 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35194]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50156]\n",
      "['has' 'had' 'could' 'can' 'have' 'GROUND']\n",
      "ground token 45592\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 13943\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 1266\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 287\n",
      "decoded ground word  in\n",
      "ground word prob is:  0.84284127\n",
      "sorted indices [ 287  319  286 6776  262  284 1683  508 1088  290]\n",
      "ten sps values [0.84284127 0.08912214 0.02224007 0.00960907 0.00953447 0.00389291\n",
      " 0.00319098 0.0022583  0.00163345 0.0014948 ]\n",
      "prob slices: [0.84284127 0.67427301 0.50570476 0.33713651 0.16856825 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35194]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50145]\n",
      "['in' 'on' 'of' 'alive' 'the' 'LEASE']\n",
      "ground token 1406\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 2084\n",
      "decoded ground word  ago\n",
      "ground word prob is:  0.75072527\n",
      "sorted indices [2084  736  878 2961  706 3161  656 1568  287 2180]\n",
      "ten sps values [0.75072527 0.06238769 0.05910664 0.03373719 0.02528925 0.02364379\n",
      " 0.00643768 0.00462693 0.00324685 0.00196383]\n",
      "prob slices: [0.75072527 0.60058022 0.45043516 0.30029011 0.15014505 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35194]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50146]\n",
      "['ago' 'back' 'before' 'earlier' 'after' 'NEWS']\n",
      "ground token 13\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 5338\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 973\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 220\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 760\n",
      "decoded ground word  know\n",
      "ground word prob is:  0.027079063\n",
      "sorted indices [ 460 1101  716  991  423 1254  760 3285  836 2314]\n",
      "ten sps values [0.20402168 0.17490853 0.14912899 0.08578479 0.05179041 0.02759262\n",
      " 0.02707906 0.02610521 0.02128876 0.0207662 ]\n",
      "prob slices: [0.20402168 0.16321734 0.12241301 0.08160867 0.04080434 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35195]\n",
      "absolute positions of slice words [0, 2, 3, 4, 5, 50143]\n",
      "['can' 'am' 'still' 'have' 'feel' 'conservancy']\n",
      "ground token 262\n",
      "decoded ground word  the\n",
      "ground word prob is:  0.32146803\n",
      "sorted indices [  257   262   616   281   617   465   530 10598   428   326]\n",
      "ten sps values [0.44043052 0.32146803 0.08462332 0.03692335 0.01487178 0.00647565\n",
      " 0.0057577  0.00531165 0.00411859 0.00404464]\n",
      "prob slices: [0.44043052 0.35234442 0.26425831 0.17617221 0.0880861  0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35196]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50151]\n",
      "['a' 'the' 'my' 'an' 'some' 'Alternative']\n",
      "ground token 938\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 674\n",
      "decoded ground word  our\n",
      "ground word prob is:  0.03929912\n",
      "sorted indices [ 547  262  550  286  674  616   11 6304  290 3114]\n",
      "ten sps values [0.38491347 0.22930746 0.09021001 0.08391155 0.03929916 0.01991919\n",
      " 0.00976702 0.00854576 0.00747028 0.00606062]\n",
      "prob slices: [0.38491347 0.30793078 0.23094808 0.15396539 0.07698269 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35193]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50149]\n",
      "['were' 'the' 'had' 'of' 'our' 'archives']\n",
      "ground token 510\n",
      "decoded ground word  up\n",
      "ground word prob is:  0.29023775\n",
      "sorted indices [994 510 656 284 287 428 326 736 422 416]\n",
      "ten sps values [0.46162358 0.29023775 0.08129427 0.07268722 0.03148916 0.01048421\n",
      " 0.00503392 0.00479656 0.00479413 0.00466667]\n",
      "prob slices: [0.46162358 0.36929886 0.27697415 0.18464943 0.09232472 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35195]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50160]\n",
      "['here' 'up' 'into' 'to' 'in' 'Cosponsors']\n",
      "ground token 247\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 703\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 477\n",
      "decoded ground word  all\n",
      "ground word prob is:  0.11724809\n",
      "sorted indices [  345 49124   477  3668  2506   262   606   514 13647   534]\n",
      "ten sps values [0.27530676 0.25284997 0.11724809 0.05485898 0.03810996 0.03300732\n",
      " 0.02770692 0.02410196 0.00962026 0.00940766]\n",
      "prob slices: [0.27530676 0.22024541 0.16518406 0.1101227  0.05506135 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35196]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50140]\n",
      "['you' 'Dirt' 'all' 'Earth' 'everyone' 'hawks']\n",
      "ground token 4001\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 3668\n",
      "decoded ground word  Earth\n",
      "ground word prob is:  0.07376727\n",
      "sorted indices [  262   674  3668   257  8706 24118   309  4534   530   616]\n",
      "ten sps values [0.32149562 0.10103543 0.07376727 0.04393134 0.02474849 0.01037354\n",
      " 0.00991308 0.00973555 0.00923784 0.00833134]\n",
      "prob slices: [0.32149562 0.2571965  0.19289737 0.12859825 0.06429912 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35193]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50137]\n",
      "['the' 'our' 'Earth' 'a' 'Mars' 'oiler']\n",
      "ground token 11\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 329\n",
      "decoded ground word  for\n",
      "ground word prob is:  0.6070613\n",
      "sorted indices [ 329  523  284  287  257  878  355  588  503 4953]\n",
      "ten sps values [0.6070613  0.0768423  0.0664531  0.03440844 0.02063576 0.01949295\n",
      " 0.01923008 0.01086602 0.01084875 0.00927434]\n",
      "prob slices: [0.60706133 0.48564906 0.3642368  0.24282453 0.12141227 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35195]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50122]\n",
      "['for' 'so' 'to' 'in' 'a' 'avid']\n",
      "ground token 11691\n",
      "decoded ground word  suppose\n",
      "ground word prob is:  0.024457293\n",
      "sorted indices [ 460  423  466  836  750 1101  564  716  714  373]\n",
      "ten sps values [0.07606108 0.05641991 0.04608597 0.04182966 0.04155849 0.04002976\n",
      " 0.03860511 0.03834723 0.03069897 0.02948304]\n",
      "prob slices: [0.07606108 0.06084886 0.04563665 0.03042443 0.01521222 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 6, 12, 35193]\n",
      "absolute positions of slice words [0, 1, 2, 8, 14, 50139]\n",
      "['can' 'have' 'do' 'could' 'think' 'rift']\n",
      "ground token 835\n",
      "decoded ground word  way\n",
      "ground word prob is:  0.004808159\n",
      "sorted indices [ 262  356  611 1637  612  428  345   11  674  340]\n",
      "ten sps values [0.1323361  0.1235096  0.07650616 0.05987259 0.04175181 0.04050183\n",
      " 0.03232007 0.03208743 0.02969613 0.02198435]\n",
      "prob slices: [0.13233609 0.10586888 0.07940166 0.05293444 0.02646722 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 7, 35194]\n",
      "absolute positions of slice words [0, 1, 2, 3, 8, 50139]\n",
      "['the' 'we' 'if' 'money' 'our' 'Crew']\n",
      "ground token 661\n",
      "decoded ground word  people\n",
      "ground word prob is:  0.014425957\n",
      "sorted indices [262 612 428 340 287  11 257 314 645 611]\n",
      "ten sps values [0.17950313 0.04289242 0.03677039 0.02908443 0.02545756 0.02509615\n",
      " 0.02177272 0.02076795 0.02065639 0.01921988]\n",
      "prob slices: [0.17950313 0.1436025  0.10770188 0.07180125 0.03590063 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35196]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50161]\n",
      "['the' 'there' 'this' 'it' 'in' 'EVA']\n",
      "ground token 470\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 290\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 18755\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 2063\n",
      "decoded ground word  half\n",
      "ground word prob is:  0.05205488\n",
      "sorted indices [ 1767  2063   530   636 28668   373  3354  3211  1735  6903]\n",
      "ten sps values [0.7409587  0.05205488 0.04751921 0.03509087 0.01166612 0.0096747\n",
      " 0.00464597 0.00447389 0.00441072 0.00419448]\n",
      "prob slices: [0.74095869 0.59276695 0.44457521 0.29638348 0.14819174 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35194]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50144]\n",
      "['body' 'half' 'one' 'part' 'torso' 'Attend']\n",
      "ground token 220\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 510\n",
      "decoded ground word  up\n",
      "ground word prob is:  0.2094082\n",
      "sorted indices [262 510 465 257 616 428 281 502 326 340]\n",
      "ten sps values [0.2931285  0.2094082  0.138198   0.12856188 0.10179077 0.02001203\n",
      " 0.01047786 0.0081699  0.00804885 0.00793408]\n",
      "prob slices: [0.29312849 0.23450279 0.17587709 0.1172514  0.0586257  0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 5, 35193]\n",
      "absolute positions of slice words [0, 1, 2, 3, 5, 50141]\n",
      "['the' 'up' 'his' 'a' 'this' 'yip']\n",
      "ground token 11\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 3155\n",
      "decoded ground word  couple\n",
      "ground word prob is:  0.10931101\n",
      "sorted indices [1178  614 3155  981 1285 1227 1310  890 1545 1110]\n",
      "ten sps values [0.52035546 0.13517702 0.10931101 0.03870826 0.02870262 0.02648702\n",
      " 0.02137477 0.01098621 0.00622781 0.00516686]\n",
      "prob slices: [0.52035546 0.41628437 0.31221328 0.20814219 0.10407109 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35194]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50144]\n",
      "['few' 'year' 'couple' 'while' 'week' 'Engineers']\n",
      "ground token 10825\n",
      "decoded ground word  emotions\n",
      "ground word prob is:  0.000914615\n",
      "sorted indices [11561  3554 23071  4365 22576  4028  2457  8492 13353   898]\n",
      "ten sps values [0.22419132 0.06905108 0.03100617 0.0284886  0.02371871 0.01917993\n",
      " 0.01727536 0.01232516 0.01089239 0.00999017]\n",
      "prob slices: [0.22419132 0.17935306 0.13451479 0.08967653 0.04483826 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35193]\n",
      "absolute positions of slice words [0, 1, 3, 4, 5, 50128]\n",
      "['commander' 'leader' 'mission' 'commanders' 'actions' 'yip']\n",
      "ground token 883\n",
      "decoded ground word  those\n",
      "ground word prob is:  0.00096519623\n",
      "sorted indices [ 338  314  318  484  262  340  428  616 3022  345]\n",
      "ten sps values [0.29192024 0.21780199 0.18371102 0.06548033 0.02470043 0.01801245\n",
      " 0.01782407 0.0167475  0.01572491 0.01357089]\n",
      "prob slices: [0.29192024 0.2335362  0.17515215 0.1167681  0.05838405 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35193]\n",
      "absolute positions of slice words [1, 2, 3, 4, 5, 50144]\n",
      "['I' 'is' 'they' 'the' 'it' 'Export']\n",
      "ground token 2067\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 198\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 362\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 340\n",
      "decoded ground word  it\n",
      "ground word prob is:  0.34888038\n",
      "sorted indices [ 340  262 1497  465  379  257  617  572  326  510]\n",
      "ten sps values [0.34888038 0.29307398 0.11994193 0.11213859 0.06299218 0.01479714\n",
      " 0.0105058  0.00522322 0.00519623 0.00193806]\n",
      "prob slices: [0.34888038 0.2791043  0.20932823 0.13955215 0.06977608 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 4, 35193]\n",
      "absolute positions of slice words [0, 1, 2, 3, 4, 50145]\n",
      "['it' 'the' 'away' 'his' 'at' 'Subscribe']\n",
      "ground token 8601\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 5822\n",
      "decoded ground word  king\n",
      "ground word prob is:  0.119537115\n",
      "sorted indices [ 5822  6181 19716  4252 10441  9234  2933  3367  2677 20589]\n",
      "ten sps values [0.11953712 0.0666927  0.0541876  0.0373808  0.02474365 0.02369402\n",
      " 0.01963837 0.01391213 0.01336329 0.01194685]\n",
      "prob slices: [0.11953712 0.09562969 0.07172227 0.04781485 0.02390742 0.        ]\n",
      "words wanted rel positions [0, 1, 2, 3, 5, 35196]\n",
      "absolute positions of slice words [0, 1, 2, 3, 5, 50161]\n",
      "['king' 'evil' 'prince' 'sun' 'monster' 'SHIP']\n",
      "ground token 11\n",
      "one of the three tokens in front were not words so skipping\n",
      "ground token 11\n",
      "one of the three tokens in front were not words so skipping\n"
     ]
    }
   ],
   "source": [
    "# applying all analyses to the logits. \n",
    "prompt_length = 100\n",
    "slices_from_each = 6 # as one will also be the ground truth word. \n",
    "to_df = []\n",
    "\n",
    "# randomly select generations and positions in those generations. from the original prompts. \n",
    "for r_ind in range(0, num_prompts_and_timepoints_wanted):\n",
    "    \n",
    "    if want_min_max ==True:\n",
    "        prompt = 999\n",
    "        batch_ind, ind_in_batch = minmax.iloc[r_ind, 1], minmax.iloc[r_ind, 2]\n",
    "        time_point = minmax.iloc[r_ind,3]\n",
    "        if time_point>=149:\n",
    "            continue # because of the stop token being predicted. \n",
    "        text_timepoint = time_point+prompt_length+1 # +1 because the perplexities keeps the end prediction token. \n",
    "    else:\n",
    "        prompt = rand_prompts[r_ind]\n",
    "        time_point = rand_times[r_ind]\n",
    "        text_timepoint = time_point+prompt_length+1 # +1 because the perplexities keeps the end prediction token. \n",
    "        batch_ind, ind_in_batch = mapping_to_batch[prompt]\n",
    "    \n",
    "    sps_no_sort = softmax(all_logits[batch_ind][ind_in_batch, time_point, :])\n",
    "    sps = softmax(-np.sort(-all_logits[batch_ind][ind_in_batch, time_point, :]))\n",
    "    prob_slices_wanted = np.linspace(sps[0],0,slices_from_each)\n",
    "    \n",
    "    indices = np.argsort(-all_logits[batch_ind][ind_in_batch, time_point, :])\n",
    "    ground_token = text[batch_ind][ind_in_batch][text_timepoint]\n",
    "    print('ground token', ground_token)\n",
    "    prev_token = text[batch_ind][ind_in_batch][text_timepoint-1]\n",
    "    prev_prev_token = text[batch_ind][ind_in_batch][text_timepoint-2]\n",
    "    prev_tokens = [ground_token, prev_token,prev_prev_token]\n",
    "    \n",
    "    # check if these are even real words first \n",
    "    real_prompt_words, _ = remove_non_words(prev_tokens, wantPrint=False)\n",
    "    #print(real_prompt_words)\n",
    "    if len(real_prompt_words)<len(prev_tokens):\n",
    "        print('one of the three tokens in front were not words so skipping')\n",
    "        continue\n",
    "        \n",
    "    else: \n",
    "        ground_word = decoder_text([ground_token])\n",
    "        print('decoded ground word', ground_word)\n",
    "        #stripped_prev_tokens.append(space_free[0])\n",
    "\n",
    "        #ground_word = stripped_prev_tokens[-1]]) #[ground_token])\n",
    "        # what the ground token is\n",
    "        ground_word_prob = sps_no_sort[ground_token]\n",
    "        \n",
    "        print('ground word prob is: ', ground_word_prob)\n",
    "        print('sorted indices', indices[:10])\n",
    "    \n",
    "    '''stripped_prev_tokens = []\n",
    "    # need to strip the spaces and recode. \n",
    "    for prev_tind, t in enumerate(prev_tokens):\n",
    "        print('token pre space correction -----',t)\n",
    "        \n",
    "        gword = decoder_text([t])\n",
    "        \n",
    "        space_free = enc.encode(gword.strip())\n",
    "        \n",
    "        \n",
    "        if len(space_free) == 0:\n",
    "            stripped_prev_tokens.append(0)\n",
    "        else: \n",
    "            stripped_prev_tokens.append(space_free[0])\n",
    "            print('what word it now decodes to -----', decoder_text([space_free[0]]))\n",
    "        \n",
    "    #print('prev tokens', prev_tokens)\n",
    "    print(stripped_prev_tokens)'''\n",
    "    \n",
    "    #clean the ground token:\n",
    "    \n",
    "    #gword = decoder_text([ground_token])\n",
    "    #print('decoded original ground word ----', gword)\n",
    "    #space_free_enc = enc.encode(gword.strip())\n",
    "    #print('space free encoded token ---- ', space_free_enc)\n",
    "    \n",
    "    #if len(space_free_enc) == 0:\n",
    "    #    continue\n",
    "    \n",
    "    #get the 15 words in front. \n",
    "    lead_prompt_words = decoder_text(text[batch_ind][ind_in_batch][text_timepoint-leading_prompt:text_timepoint])\n",
    "    df_row = [prompt, time_point, batch_ind, ind_in_batch, lead_prompt_words, ground_word, ground_word_prob] # taking it from the last one. \n",
    "    \n",
    "    print('ten sps values', sps[0:10])\n",
    "    print('prob slices:', prob_slices_wanted)\n",
    "    \n",
    "    #remove any non word tokens from whole list. (could be done more efficiently)\n",
    "    real_word_tokens, abs_pos = remove_non_words(indices)\n",
    "    \n",
    "    real_word_probs = sps[np.asarray(abs_pos)]\n",
    "    \n",
    "    rel_to_abs_index = {rel:absolute for rel, absolute in zip(range(len(abs_pos)), abs_pos )}\n",
    "    \n",
    "    # select the right positions and words: \n",
    "    #NEED TO IGNORE ANY OF THE SLICES THAT ARE TOO LARGE.  TAKE THE HIGHEST PROB WORD AND THEN DO THE SLICES.\n",
    "    rel_words_positions_wanted, sel_word_probs = get_specific_positions_from_probs(real_word_probs, prob_slices_wanted)\n",
    "    \n",
    "    print('words wanted rel positions', rel_words_positions_wanted)\n",
    "    \n",
    "    abs_words_positions_wanted = [ rel_to_abs_index[rel] for rel in rel_words_positions_wanted]\n",
    "    print('absolute positions of slice words', abs_words_positions_wanted)\n",
    "    print(np.asarray(real_word_tokens)[np.asarray(rel_words_positions_wanted)])\n",
    "    df_row.append(np.asarray(real_word_tokens)[np.asarray(rel_words_positions_wanted)]) # adding the words themselves. \n",
    "    df_row.append(abs_words_positions_wanted)\n",
    "    df_row.append(sel_word_probs)\n",
    "        \n",
    "    to_df.append(df_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = enchant.Dict(\"en_US\")\n",
    "d.check('rust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(to_df, columns = ['prompt_ind', 'time_point', \n",
    "                               'batch_ind', 'ind_in_batch', 'leading_words',\n",
    "                               'ground_word', 'ground_word_prob', 'words', 'abs_inds', 'probs'])\n",
    "                                         \n",
    "results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert results.drop_duplicates('leading_words').shape == results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_ind</th>\n",
       "      <th>time_point</th>\n",
       "      <th>batch_ind</th>\n",
       "      <th>ind_in_batch</th>\n",
       "      <th>leading_words</th>\n",
       "      <th>ground_word</th>\n",
       "      <th>ground_word_prob</th>\n",
       "      <th>words</th>\n",
       "      <th>abs_inds</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>\\n In addition, the new sorority of Faeries, ...</td>\n",
       "      <td>Princess</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>[Demons, Wizards, Witches, Ghosts, Devils, cum...</td>\n",
       "      <td>[0, 1, 2, 3, 6, 50152]</td>\n",
       "      <td>[0.09158789, 0.036080822, 0.034813635, 0.02311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>about your Master, Jamie, Mickeymouse?'' Lewi...</td>\n",
       "      <td>his</td>\n",
       "      <td>0.110115</td>\n",
       "      <td>[to, not, the, his, a, abase]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 50143]</td>\n",
       "      <td>[0.3743754, 0.15168922, 0.114058, 0.1101153, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>wronged him all the while overpaying for all ...</td>\n",
       "      <td>has</td>\n",
       "      <td>0.804644</td>\n",
       "      <td>[has, had, could, can, have, GROUND]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 50156]</td>\n",
       "      <td>[0.8046437, 0.11674176, 0.021794733, 0.0142553...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>Alexandre Dumas, who at one point was one of ...</td>\n",
       "      <td>in</td>\n",
       "      <td>0.842841</td>\n",
       "      <td>[in, on, of, alive, the, LEASE]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 50145]</td>\n",
       "      <td>[0.84284127, 0.08912214, 0.022240072, 0.009609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>\\n \\n I was not always this way, just a few m...</td>\n",
       "      <td>ago</td>\n",
       "      <td>0.750725</td>\n",
       "      <td>[ago, back, before, earlier, after, NEWS]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 50146]</td>\n",
       "      <td>[0.75072527, 0.062387686, 0.059106637, 0.03373...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n \\n \\n Oh no, my eyes may be closed but I</td>\n",
       "      <td>know</td>\n",
       "      <td>0.027079</td>\n",
       "      <td>[can, am, still, have, feel, conservancy]</td>\n",
       "      <td>[0, 2, 3, 4, 5, 50143]</td>\n",
       "      <td>[0.20402168, 0.14912899, 0.08578479, 0.0517904...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78</td>\n",
       "      <td>139</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>all over Rome, and i get to fish it out of th...</td>\n",
       "      <td>the</td>\n",
       "      <td>0.321468</td>\n",
       "      <td>[a, the, my, an, some, Alternative]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 50151]</td>\n",
       "      <td>[0.44043052, 0.32146803, 0.08462332, 0.0369233...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_ind  time_point  batch_ind  ind_in_batch  \\\n",
       "0          22         125          0            22   \n",
       "1          87         130          3            17   \n",
       "2          12          31          0            12   \n",
       "3          12          72          0            12   \n",
       "4           8         139          0             8   \n",
       "5           1          41          0             1   \n",
       "6          78         139          3             8   \n",
       "\n",
       "                                       leading_words ground_word  \\\n",
       "0   \\n In addition, the new sorority of Faeries, ...    Princess   \n",
       "1   about your Master, Jamie, Mickeymouse?'' Lewi...         his   \n",
       "2   wronged him all the while overpaying for all ...         has   \n",
       "3   Alexandre Dumas, who at one point was one of ...          in   \n",
       "4   \\n \\n I was not always this way, just a few m...         ago   \n",
       "5        \\n \\n \\n Oh no, my eyes may be closed but I        know   \n",
       "6   all over Rome, and i get to fish it out of th...         the   \n",
       "\n",
       "   ground_word_prob                                              words  \\\n",
       "0          0.000811  [Demons, Wizards, Witches, Ghosts, Devils, cum...   \n",
       "1          0.110115                      [to, not, the, his, a, abase]   \n",
       "2          0.804644               [has, had, could, can, have, GROUND]   \n",
       "3          0.842841                    [in, on, of, alive, the, LEASE]   \n",
       "4          0.750725          [ago, back, before, earlier, after, NEWS]   \n",
       "5          0.027079          [can, am, still, have, feel, conservancy]   \n",
       "6          0.321468                [a, the, my, an, some, Alternative]   \n",
       "\n",
       "                 abs_inds                                              probs  \n",
       "0  [0, 1, 2, 3, 6, 50152]  [0.09158789, 0.036080822, 0.034813635, 0.02311...  \n",
       "1  [0, 1, 2, 3, 4, 50143]  [0.3743754, 0.15168922, 0.114058, 0.1101153, 0...  \n",
       "2  [0, 1, 2, 3, 4, 50156]  [0.8046437, 0.11674176, 0.021794733, 0.0142553...  \n",
       "3  [0, 1, 2, 3, 4, 50145]  [0.84284127, 0.08912214, 0.022240072, 0.009609...  \n",
       "4  [0, 1, 2, 3, 4, 50146]  [0.75072527, 0.062387686, 0.059106637, 0.03373...  \n",
       "5  [0, 2, 3, 4, 5, 50143]  [0.20402168, 0.14912899, 0.08578479, 0.0517904...  \n",
       "6  [0, 1, 2, 3, 4, 50151]  [0.44043052, 0.32146803, 0.08462332, 0.0369233...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n \\n \\n Oh no, my eyes may be closed but I'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.loc[5, 'leading_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_base = 'MTURK_QUESTIONS_TWO_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(file_name_base+'_backup_of_different_prob_slices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_orders = []\n",
    "with open(file_name_base + '_blind_output.txt', 'w') as file: \n",
    "    file.write('For each of the following assignments, rank on a scale of 1-5 how possible each word is, given the provided context. \\n')\n",
    "    file.write('Some words are stems, if the word could be completed then you can still rank it highly. And be generous with acronyms that could be plausible. ')\n",
    "    file.write('It helps to repeat the last few words of the prompt in your head while deciding how replaceable the current one is.')\n",
    "    file.write('5 means the word would work very well here, 1 means it doesnt make any sense, 3 means it could potentially work.')\n",
    "    file.write('===================================================== \\n \\n')\n",
    "    for ind in range(results.shape[0]):\n",
    "        file.write('Prompt '+str(ind)+'. \\n')\n",
    "        file.write('The '+str(leading_prompt)+' words in front: \\n \\n')\n",
    "        file.write(results.loc[ind, 'leading_words']+' _______')\n",
    "        #file.write(results.loc[ind, 'leading_words']+' : '+ results.loc[ind, 'ground_word'] )\n",
    "        file.write('\\n')\n",
    "        \n",
    "        words = [results.loc[ind, 'ground_word']]\n",
    "        words += list(results.loc[ind, 'words'])\n",
    "        words =np.asarray(words)\n",
    "        \n",
    "        order = ['ground_word']\n",
    "        order += [str(i) for i in range(len(words)-1)]\n",
    "        order = np.asarray(order)\n",
    "        \n",
    "        probs = [results.loc[ind, 'ground_word_prob']]\n",
    "        probs += results.loc[ind, 'probs']\n",
    "        probs = np.asarray(probs)\n",
    "                \n",
    "        #print(words, order)\n",
    "        shuffler = np.random.choice(range(len(words)), size =len(words), replace=False )\n",
    "        #print(len(words))\n",
    "        #print(shuffler)\n",
    "        \n",
    "        words = words[shuffler]\n",
    "        order = order[shuffler]\n",
    "        probs = probs[shuffler]\n",
    "        \n",
    "        question_answers = []\n",
    "        for w, p, o in zip(words,probs, order):\n",
    "            \n",
    "            if o=='ground_word':\n",
    "                w_p = (w,p,1)\n",
    "            else: \n",
    "                w_p = (w,p,0)\n",
    "        \n",
    "            question_answers.append(w_p)\n",
    "            \n",
    "        answer_orders.append(question_answers)\n",
    "        \n",
    "        #print('after', words, order)\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            file.write(str(i+1)+'. '+words[i]+' ')\n",
    "                \n",
    "        file.write('\\n \\n')\n",
    "        file.write('=====================================================')\n",
    "        file.write('\\n \\n')\n",
    "        \n",
    "with open(file_name_base + '_answers.txt', 'w') as file: \n",
    "    for ind, w_p in enumerate(answer_orders):\n",
    "        file.write('Prompt '+str(ind)+' : \\n')\n",
    "        for el_ind, el in enumerate(w_p): \n",
    "            file.write(str(el_ind+1) + '. '+str(el) +' ')\n",
    "        file.write('\\n')\n",
    "        file.write('=====================================================')\n",
    "        file.write('\\n')\n",
    "\n",
    "pickle.dump(answer_orders,open(file_name_base + '_answers_list.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Devils', 0.019027840346097946, 0),\n",
       "  (' Princess', 0.0008109641494229436, 1),\n",
       "  ('cumbers', 1.6756467741130265e-11, 0),\n",
       "  ('Demons', 0.09158789366483688, 0),\n",
       "  ('Witches', 0.03481363505125046, 0),\n",
       "  ('Wizards', 0.036080822348594666, 0),\n",
       "  ('Ghosts', 0.023113250732421875, 0)],\n",
       " [('his', 0.11011529713869095, 0),\n",
       "  ('the', 0.1140580028295517, 0),\n",
       "  ('a', 0.057745471596717834, 0),\n",
       "  ('to', 0.3743754029273987, 0),\n",
       "  ('abase', 1.9319823518770818e-11, 0),\n",
       "  ('not', 0.15168921649456024, 0),\n",
       "  (' his', 0.11011529713869095, 1)],\n",
       " [(' has', 0.8046436905860901, 1),\n",
       "  ('could', 0.02179473266005516, 0),\n",
       "  ('GROUND', 1.9108309769547138e-13, 0),\n",
       "  ('has', 0.8046436905860901, 0),\n",
       "  ('can', 0.01425535324960947, 0),\n",
       "  ('have', 0.004054101649671793, 0),\n",
       "  ('had', 0.11674176156520844, 0)],\n",
       " [('the', 0.009534470736980438, 0),\n",
       "  ('LEASE', 3.4826590396275225e-13, 0),\n",
       "  ('alive', 0.009609066881239414, 0),\n",
       "  ('on', 0.08912213891744614, 0),\n",
       "  ('in', 0.8428412675857544, 0),\n",
       "  ('of', 0.02224007248878479, 0),\n",
       "  (' in', 0.8428412675857544, 1)],\n",
       " [('after', 0.025289252400398254, 0),\n",
       "  ('back', 0.06238768622279167, 0),\n",
       "  ('before', 0.05910663679242134, 0),\n",
       "  ('earlier', 0.033737193793058395, 0),\n",
       "  (' ago', 0.750725269317627, 1),\n",
       "  ('ago', 0.750725269317627, 0),\n",
       "  ('NEWS', 3.762782620209126e-12, 0)],\n",
       " [(' know', 0.027079062536358833, 1),\n",
       "  ('can', 0.20402167737483978, 0),\n",
       "  ('conservancy', 2.7998113810018266e-12, 0),\n",
       "  ('am', 0.1491289883852005, 0),\n",
       "  ('feel', 0.0275926161557436, 0),\n",
       "  ('still', 0.08578479290008545, 0),\n",
       "  ('have', 0.05179040879011154, 0)],\n",
       " [('Alternative', 8.699211490048597e-12, 0),\n",
       "  ('a', 0.44043052196502686, 0),\n",
       "  (' the', 0.3214680254459381, 1),\n",
       "  ('some', 0.014871776103973389, 0),\n",
       "  ('an', 0.036923352628946304, 0),\n",
       "  ('the', 0.3214680254459381, 0),\n",
       "  ('my', 0.084623321890831, 0)],\n",
       " [('our', 0.03929915651679039, 0),\n",
       "  ('archives', 2.596707354349248e-12, 0),\n",
       "  ('were', 0.38491347432136536, 0),\n",
       "  ('the', 0.22930745780467987, 0),\n",
       "  (' our', 0.039299119263887405, 1),\n",
       "  ('had', 0.09021001309156418, 0),\n",
       "  ('of', 0.08391154557466507, 0)],\n",
       " [('Cosponsors', 1.4321238693635469e-13, 0),\n",
       "  ('in', 0.031489163637161255, 0),\n",
       "  ('up', 0.29023775458335876, 0),\n",
       "  (' up', 0.29023775458335876, 1),\n",
       "  ('here', 0.46162357926368713, 0),\n",
       "  ('to', 0.07268722355365753, 0),\n",
       "  ('into', 0.08129426836967468, 0)],\n",
       " [('you', 0.275306761264801, 0),\n",
       "  ('Earth', 0.0548589788377285, 0),\n",
       "  ('everyone', 0.03810995817184448, 0),\n",
       "  ('all', 0.11724808812141418, 0),\n",
       "  ('hawks', 6.242212476081921e-12, 0),\n",
       "  (' all', 0.11724808812141418, 1),\n",
       "  ('Dirt', 0.2528499662876129, 0)],\n",
       " [('Earth', 0.0737672671675682, 0),\n",
       "  ('our', 0.10103543102741241, 0),\n",
       "  ('the', 0.3214956223964691, 0),\n",
       "  (' Earth', 0.0737672671675682, 1),\n",
       "  ('oiler', 3.9182029148987496e-11, 0),\n",
       "  ('a', 0.04393133893609047, 0),\n",
       "  ('Mars', 0.024748489260673523, 0)],\n",
       " [('for', 0.6070613265037537, 0),\n",
       "  ('to', 0.06645309925079346, 0),\n",
       "  (' for', 0.6070613265037537, 1),\n",
       "  ('so', 0.076842300593853, 0),\n",
       "  ('avid', 2.2938163955071866e-12, 0),\n",
       "  ('in', 0.034408438950777054, 0),\n",
       "  ('a', 0.020635761320590973, 0)],\n",
       " [('have', 0.05641990527510643, 0),\n",
       "  ('could', 0.03069896809756756, 0),\n",
       "  ('do', 0.04608597233891487, 0),\n",
       "  ('rift', 3.855831279264699e-11, 0),\n",
       "  ('think', 0.016215069219470024, 0),\n",
       "  (' suppose', 0.0244572926312685, 1),\n",
       "  ('can', 0.07606107741594315, 0)],\n",
       " [('Crew', 1.9417831925716555e-11, 0),\n",
       "  ('our', 0.029696132987737656, 0),\n",
       "  (' way', 0.004808159079402685, 1),\n",
       "  ('we', 0.12350960075855255, 0),\n",
       "  ('the', 0.1323360949754715, 0),\n",
       "  ('money', 0.0598725900053978, 0),\n",
       "  ('if', 0.07650616019964218, 0)],\n",
       " [('the', 0.17950312793254852, 0),\n",
       "  ('there', 0.04289241507649422, 0),\n",
       "  ('EVA', 1.9288194305633333e-11, 0),\n",
       "  (' people', 0.014425956644117832, 1),\n",
       "  ('it', 0.029084429144859314, 0),\n",
       "  ('this', 0.03677038848400116, 0),\n",
       "  ('in', 0.025457561016082764, 0)],\n",
       " [('torso', 0.011666123755276203, 0),\n",
       "  ('one', 0.04751920700073242, 0),\n",
       "  ('body', 0.7409586906433105, 0),\n",
       "  ('part', 0.035090867429971695, 0),\n",
       "  ('half', 0.05205487832427025, 0),\n",
       "  (' half', 0.05205487832427025, 1),\n",
       "  ('Attend', 4.0404121021131445e-12, 0)],\n",
       " [('his', 0.13819800317287445, 0),\n",
       "  (' up', 0.20940819382667542, 1),\n",
       "  ('the', 0.29312849044799805, 0),\n",
       "  ('a', 0.12856188416481018, 0),\n",
       "  ('yip', 7.2489145762355545e-12, 0),\n",
       "  ('up', 0.20940819382667542, 0),\n",
       "  ('this', 0.020012028515338898, 0)],\n",
       " [(' couple', 0.10931100696325302, 1),\n",
       "  ('year', 0.13517701625823975, 0),\n",
       "  ('week', 0.028702618554234505, 0),\n",
       "  ('few', 0.5203554630279541, 0),\n",
       "  ('couple', 0.10931100696325302, 0),\n",
       "  ('while', 0.03870825842022896, 0),\n",
       "  ('Engineers', 3.4705807672175126e-11, 0)],\n",
       " [('leader', 0.06905107945203781, 0),\n",
       "  ('commanders', 0.023718705400824547, 0),\n",
       "  ('yip', 9.447197191203571e-11, 0),\n",
       "  ('commander', 0.2241913229227066, 0),\n",
       "  ('actions', 0.0191799309104681, 0),\n",
       "  ('mission', 0.02848859690129757, 0),\n",
       "  (' emotions', 0.0009146149968728423, 1)],\n",
       " [('the', 0.024700434878468513, 0),\n",
       "  ('is', 0.18371102213859558, 0),\n",
       "  ('they', 0.06548032909631729, 0),\n",
       "  ('it', 0.018012449145317078, 0),\n",
       "  (' those', 0.0009651962318457663, 1),\n",
       "  ('I', 0.2178019881248474, 0),\n",
       "  ('Export', 3.75291984988646e-12, 0)],\n",
       " [('at', 0.06299217790365219, 0),\n",
       "  (' it', 0.3488803803920746, 1),\n",
       "  ('his', 0.11213859170675278, 0),\n",
       "  ('away', 0.11994192749261856, 0),\n",
       "  ('it', 0.3488803803920746, 0),\n",
       "  ('Subscribe', 4.4494215169915696e-13, 0),\n",
       "  ('the', 0.29307398200035095, 0)],\n",
       " [('sun', 0.03738079592585564, 0),\n",
       "  ('SHIP', 2.963615897827454e-12, 0),\n",
       "  ('evil', 0.06669270247220993, 0),\n",
       "  ('king', 0.1195371150970459, 0),\n",
       "  ('monster', 0.02369401603937149, 0),\n",
       "  (' king', 0.1195371150970459, 1),\n",
       "  ('prince', 0.0541875958442688, 0)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 3, 4, 3, 1, 4, 5, 4, 4, 2],\n",
       " [3, 2, 5, 1, 4, 5, 3, 5, 2, 4],\n",
       " [5, 4, 1, 5, 3, 5, 4, 5, 2, 2],\n",
       " [3, 5, 3, 5, 5, 3, 4, 5, 4],\n",
       " [3, 5, 2, 2, 3, 4, 4, 1, 5, 5],\n",
       " [5, 2, 1, 5, 2, 5, 4, 5, 1, 4],\n",
       " [1, 5, 5, 4, 5, 5, 5, 4, 3],\n",
       " [1, 1, 2, 1, 1, 5, 5, 5, 2, 4],\n",
       " [4, 1, 5, 5, 4, 3, 5, 3, 2, 2],\n",
       " [5, 2, 3, 4, 5, 5, 5, 5, 5, 4]]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[5, 3, 4, 3, 1, 4, 5, 4, 4, 2,], [3,2,5,1,4,5,3,5,2,4],[5,4,1,5, 3,5,4,5,2,2], [3,5,3,5,5,3,4,5,4], [3,5,2,2,3,4,4,1,5,5],[5, 2, 1,5,2,5,4,5,1,4], [1,5,5,4,5,5,5,4,3], [1,1,2,1,1,5,5,5,2,4], [4,1,5,5,4,3,5,3,2,2], [5,2,3,4,5,5,5,5,5,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for r in res: \n",
    "    print(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd = dict()\n",
    "for ind, (r, a) in enumerate(zip(res, answer_orders)):\n",
    "    for r_el, a_el in zip(r,a):\n",
    "        try:\n",
    "            prd[a_el[1]].append(r_el)\n",
    "        except:\n",
    "            prd[a_el[1]] = [r_el]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd = pd.DataFrame(prd).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.438798e-02</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.133592e-01</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.043329e-02</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.023387e-06</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.646489e-11</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "1.438798e-02  5\n",
       "1.133592e-01  1\n",
       "1.043329e-02  5\n",
       "4.023387e-06  4\n",
       "4.646489e-11  5"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probs</th>\n",
       "      <th>rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.438798e-02</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.133592e-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.043329e-02</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.023387e-06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.646489e-11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          probs  rep\n",
       "0  1.438798e-02    5\n",
       "1  1.133592e-01    1\n",
       "2  1.043329e-02    5\n",
       "3  4.023387e-06    4\n",
       "4  4.646489e-11    5"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd.reset_index(inplace=True)\n",
    "#prd.drop('index', axis=1, inplace=True)\n",
    "prd.columns = ['probs', 'rep']\n",
    "prd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "\n",
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd['rep'] = prd['rep']*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x821a70fd0>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do a scatter plot of this. \n",
    "plt.scatter(prd['probs'], prd['rep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rep</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.083610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.023622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.054777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        probs\n",
       "rep          \n",
       "2    0.081718\n",
       "4    0.025102\n",
       "6    0.083610\n",
       "8    0.023622\n",
       "10   0.054777"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = prd.groupby('rep').mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x822685400>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(means['probs'], means.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bar_plot_columns(prd, 'TFS is tighter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bar_plot_columns(nuc_tighter, 'Nuc is Tighter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want when Nuc is tighter for the difference between Nuc and TFS to be smaller than it is when TFS is tighter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: 1 = 0.3999999999999999\n",
      "position: 2 = 0.6000000000000001\n",
      "position: 3 = 0.7999999999999998\n"
     ]
    }
   ],
   "source": [
    "abs_diff_of_different_locations(tfs_tighter, tfs_label, n_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: 1 = 1.75\n",
      "position: 2 = 0.25\n",
      "position: 3 = 1.25\n"
     ]
    }
   ],
   "source": [
    "abs_diff_of_different_locations(nuc_tighter,tfs_label, n_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
