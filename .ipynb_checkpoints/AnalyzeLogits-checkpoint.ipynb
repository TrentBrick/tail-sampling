{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pandas as pd    \n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of the sequences that are too short and merge all of the logits:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=25\n",
    "num_batches=4\n",
    "prompt_length=100\n",
    "generated_length=150\n",
    "tot_len = prompt_length+generated_length\n",
    "\n",
    "import encoder\n",
    "model_name='345M'\n",
    "models_dir='../gpt-2/models'\n",
    "enc = encoder.get_encoder(model_name, models_dir)\n",
    "\n",
    "prompts=pd.read_csv('test_dataframe_500primer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated\n",
    "vals_dict = {'tfs':[0.75]  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key is: tfs\n",
      "opening file: gpt-2_output/all_logits_tfs-sampling-type_0.75-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz\n"
     ]
    }
   ],
   "source": [
    "bad_inds = []\n",
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        if par ==None:\n",
    "            par = \"None\"\n",
    "        print('opening file:', 'gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz')\n",
    "        all_logits = pickle.load( gzip.open('gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        text = pickle.load( gzip.open('gpt-2_output/all_text_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        rand_selects = pickle.load( gzip.open('gpt-2_output/prompt_rand_selections_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "\n",
    "        #clean_logits = []\n",
    "        p_ind=0\n",
    "        bad_inds =[]\n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            for ind in range(0,batch_size):\n",
    "                \n",
    "                p_ind = ind+(batch*batch_size)\n",
    "                \n",
    "                if p_ind%25 == 0:\n",
    "                    print('index', str(p_ind))\n",
    "\n",
    "                # this is the ground truth calculations =====\n",
    "\n",
    "                choose_prompt =rand_selects[p_ind]\n",
    "                prompt_target_encoded = enc.encode(prompts.iloc[choose_prompt].Prompt)\n",
    "\n",
    "                if len(prompt_target_encoded) < tot_len: # some of the prompts ground truth completions\n",
    "                    #are too short for the timepoint.\n",
    "                    #num_target_too_short+=1\n",
    "                    bad_inds.append(p_ind)\n",
    "                    print(p_ind)\n",
    "                    continue\n",
    "                    \n",
    "                #clean_logits.append(all_logits[batch][ind])\n",
    "                \n",
    "        '''print(len(clean_logits))\n",
    "        clean_logits = np.asarray(clean_logits)\n",
    "        pickle.dump(clean_logits, gzip.open('gpt-2_output/cleaned_all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'wb'))\n",
    "'''                \n",
    "bad_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad_inds are the logits to text and logits to ignore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the logits that are outputted and analyzing them. \n",
    "\n",
    "prompt_wanted = 78\n",
    "\n",
    "prompt_length = 100\n",
    "\n",
    "#vals_dict = {'tfs':[0.01 ] }\n",
    "\n",
    "#updated\n",
    "\n",
    "vals_dict = {'tfs':[0.75]  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, params in vals_dict.items():\n",
    "    for par in params:\n",
    "        print(key, par)\n",
    "        if par ==None:\n",
    "            par = \"None\"\n",
    "        all_logits = pickle.load( gzip.open('gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        text = pickle.load( gzip.open('gpt-2_output/all_text_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        rand_selects = pickle.load( gzip.open('gpt-2_output/prompt_rand_selections_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_selects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches=4\n",
    "tot_num = 0\n",
    "for i in range(num_batches):\n",
    "    tot_num+= all_logits[i].shape[0]\n",
    "print('total number of samples:', tot_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_selects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_dict = {'tfs':[0.25, 0.75, 0.9, 0.95, 0.99], 'flat':[0.01, 0.02, 0.05],\n",
    "'n': [0.1, 0.25, 0.5, 0.75, 0.9], 'k':[1,10,40,200]  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to decode:\n",
    "batch_size = 25\n",
    "from decodeLogits import *\n",
    "batch = prompt_wanted//batch_size\n",
    "ind = prompt_wanted - batch*batch_size\n",
    "\n",
    "print(batch, ind)\n",
    "\n",
    "tokens = []\n",
    "for time_point in range(all_logits[0].shape[2]):\n",
    "    tokens.append(np.argmax(all_logits[batch][ind, :, time_point]))\n",
    "decoder_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_prompt =rand_selects[prompt_wanted]\n",
    "choose_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts.iloc[choose_prompt].Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts.iloc[choose_prompt].test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_target_encoded = enc.encode(prompts.iloc[choose_prompt].Prompt)\n",
    "target = prompt_target_encoded[prompt_length:] # encode and get the prompt length. The rest is the ground truth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_text(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.encode('dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pandas as pd    \n",
    "import torch\n",
    "\n",
    "#time_point = 1\n",
    "plot_window_lim =30\n",
    "\n",
    "def ema_eff(alpha,  vals, perc_acc=0.99 ):\n",
    "    k = int(np.log(1-perc_acc)/np.log(1-alpha)) # this should be calculated at the start not in the loop!! \n",
    "    \n",
    "    if k>vals.shape[0]: # CHECK THIS SHAPE MEASUREMENT\n",
    "        k = vals.shape[0]\n",
    "    \n",
    "    # have something to check the tail id is less than the window size. \n",
    "    \n",
    "    window_weights = (1-alpha)**np.arange(0,k)\n",
    "    p = k-1\n",
    "    # THIS CAN BE DONE IN A BATCH V EFFICIENTLY\n",
    "    out = torch.nn.functional.conv1d(torch.from_numpy(vals).unsqueeze(0).unsqueeze(1).double(),torch.from_numpy(window_weights).unsqueeze(0).unsqueeze(1), padding=p )\n",
    "    out = alpha*out[0,0,p:]\n",
    "    #print(out.shape)\n",
    "    \n",
    "    return out.numpy()\n",
    "\n",
    "\n",
    "print('Prompt: \\n')\n",
    "print(decoder_text( text[batch][ind, :prompt_length]))\n",
    "print('\\n ====== \\n ')\n",
    "print('Generation: \\n ')\n",
    "gen = text[batch][ind, prompt_length:]\n",
    "print(decoder_text( text[batch][ind, prompt_length:]))\n",
    "print('======')\n",
    "\n",
    "target_prob_assigned = []\n",
    "\n",
    "for time_point in range(0,50):\n",
    "\n",
    "    sps = softmax(-np.sort(-all_logits[batch][ind, :, time_point]))\n",
    "    indices = np.argsort(-all_logits[batch][ind, :, time_point])\n",
    "\n",
    "    first = sps[1:] - sps[:-1]\n",
    "    second = first[1:] - first[:-1]\n",
    "    tail_id = second.shape[0]-np.argmax(np.flip(second)>0.001)    \n",
    "    \n",
    "    target_plot_index = np.where(target[time_point]==indices)[0][0] # this is the position in the indices where the word matches. \n",
    "    \n",
    "    print('target word', decoder_text([target[time_point]]))\n",
    "\n",
    "    #finding the tail without any alpha first\n",
    "    ids_above_tail = indices[:tail_id] # use these indices to determine what the words are and their probabilities. \n",
    "    tail_free_probs = softmax(all_logits[batch][ind, ids_above_tail, time_point]) # it is already applying the softmax too\n",
    "    \n",
    "    target_prob_assigned.append(sps[target_plot_index]) # storing the probability given to the real word. \n",
    "\n",
    "    print('tail value', tail_id)\n",
    "    print('target index', target_plot_index)\n",
    "\n",
    "    plt.plot(np.arange(sps.shape[0]),sps)\n",
    "    plt.xlim([0,plot_window_lim])\n",
    "    plt.axvline(tail_id, color='purple', linestyle='solid')\n",
    "    plt.axvline(target_plot_index, color='yellow', linestyle='dotted')\n",
    "    plt.xticks(np.arange(plot_window_lim), decoder_text(indices[:plot_window_lim]).split(' ')[1:], rotation='vertical')\n",
    "    plt.title('Tail Free Thresh')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(tail_free_probs.shape[0]), tail_free_probs)\n",
    "    plt.title('Tail Free Probs')\n",
    "    if target_plot_index in ids_above_tail:\n",
    "        plt.axvline(target_plot_index, color='yellow', linestyle='dotted')\n",
    "    plt.xticks(np.arange(tail_free_probs.shape[0]), decoder_text(ids_above_tail).split(' ')[1:], rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "    #print('words in order',decoder_text(ids_above_tail))\n",
    "\n",
    "    print('=============')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the probability assigned to the ground truth over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts.iloc[choose_prompt].test_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts.iloc[choose_prompt].test_target.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts.iloc[choose_prompt].Prompt.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_calc(vals, thresh ):\n",
    "    return np.argmax(np.cumsum(vals)>thresh)+1\n",
    "\n",
    "def new_tfs(second, thresh):\n",
    "    only_pos = np.abs(second)\n",
    "    sec_indices = np.arange(len(second))\n",
    "    sec_weights = only_pos/only_pos.sum()\n",
    "    tail_id = np.argmax(np.cumsum(sec_weights)>thresh)+1\n",
    "    return tail_id\n",
    "        \n",
    "def flat(sps, p):\n",
    "    return sps.shape[0]-np.argmax(np.flip(sps)>p)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_num = 0\n",
    "for i in range(num_batches):\n",
    "    tot_num+= all_logits[i].shape[0]\n",
    "print('total number of samples:', tot_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_good_samples = list(set(range(tot_num)) - set(bad_inds))\n",
    "tot_good_samples;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_selects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " all_logits[0].shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_logits\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pandas as pd    \n",
    "import torch\n",
    "\n",
    "num_samples = tot_num\n",
    "num_batches = num_samples//batch_size\n",
    "\n",
    "ground_token_in_cut = dict()\n",
    "ground_truth_probs = dict()\n",
    "generation_log_probs = dict()\n",
    "generation_perplexity = dict()\n",
    "\n",
    "all_tail_ids = dict()\n",
    "all_tail_cdfs = dict()\n",
    "\n",
    "num_target_too_short = 0\n",
    "\n",
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        if par ==None:\n",
    "            par = \"None\"\n",
    "        print('opening file:', 'gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz')\n",
    "        all_logits = pickle.load( gzip.open('gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        text = pickle.load( gzip.open('gpt-2_output/all_text_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        #rand_selects = pickle.load( gzip.open('gpt-2_output/prompt_rand_selections_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "\n",
    "        temp_tail_cdfs = np.zeros([len(tot_good_samples), all_logits[0].shape[2]])\n",
    "        temp_tail_ids = np.zeros([len(tot_good_samples), all_logits[0].shape[2]])\n",
    "        p_ind = 0\n",
    "        input_ind = 0 # for inputting into the array\n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            for ind in range(0,all_logits[batch].shape[0]):\n",
    "                \n",
    "                if p_ind in bad_inds:\n",
    "                    p_ind+=1\n",
    "                    continue\n",
    "                \n",
    "                if p_ind%25 == 0:\n",
    "                    print('index', str(p_ind))\n",
    "                    \n",
    "                # this is the ground truth calculations =====\n",
    "\n",
    "                choose_prompt =rand_selects[p_ind]\n",
    "                prompt_target_encoded = enc.encode(prompts.iloc[choose_prompt].Prompt)\n",
    "                target = prompt_target_encoded[prompt_length:] # encode and get the prompt length. The rest is the ground truth. \n",
    "\n",
    "                '''if len(target) < all_logits[0].shape[2]: # some of the prompts ground truth completions\n",
    "                    #are too short for the timepoint.\n",
    "                    num_target_too_short+=1\n",
    "\n",
    "                    #print('skipping as ground truth is too short for the array that I am using the store an plot the info.')\n",
    "                    continue'''\n",
    "                \n",
    "                target_prob_assigned = []\n",
    "                generation_log_probs_temp = []\n",
    "                generation_perp_temp = []\n",
    "                ground_token_in_cut_temp = []\n",
    "\n",
    "                for time_point in range(0,all_logits[0].shape[2]):\n",
    "                    #print(time_point)\n",
    "\n",
    "                    sps = softmax(all_logits[batch][ind, :, time_point])\n",
    "                    \n",
    "                    target_prob_assigned.append(sps[target[time_point]])\n",
    "                    token_chosen = text[batch][ind, prompt_length+time_point]# the text includes the starting prompt!!! \n",
    "                    \n",
    "                    log_prob_here = np.log(sps[token_chosen])\n",
    "                    generation_log_probs_temp.append(log_prob_here)\n",
    "                    \n",
    "                    #getting and storing perplexity here\n",
    "                    # correcting for zeros but only inside the log.\n",
    "                    generation_perp_temp.append(  np.power(2,  -np.sum( sps*np.log2(sps+0.000000001) ))  )\n",
    "                    \n",
    "                    sps = softmax(-np.sort(-all_logits[batch][ind, :, time_point]))\n",
    "                    indices = np.argsort(-all_logits[batch][ind, :, time_point])\n",
    "                    \n",
    "                    if key == 'tfs':\n",
    "                        first = sps[1:] - sps[:-1]\n",
    "                        second = first[1:] - first[:-1]\n",
    "                        tail_id = new_tfs(second, par)\n",
    "                    elif key=='flat':\n",
    "                        tail_id = flat(sps, par)\n",
    "                    elif key=='n':\n",
    "                        tail_id = nucleus_calc(sps, par)\n",
    "                    elif key=='k':\n",
    "                        tail_id = par\n",
    "                    else:\n",
    "                        print('key not recognized')\n",
    "                        break\n",
    "\n",
    "                    tail_cdf= np.sum(sps[:tail_id])\n",
    "\n",
    "                    temp_tail_ids[input_ind, time_point] = tail_id\n",
    "                    temp_tail_cdfs[input_ind, time_point] = tail_cdf\n",
    "                    \n",
    "                    ids_above_tail = indices[:tail_id]\n",
    "                    ground_token_in_cut_temp.append( target[time_point] in ids_above_tail )\n",
    "                    \n",
    "                ground_token_in_cut[key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)] = ground_token_in_cut_temp\n",
    "                ground_truth_probs[key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)] = target_prob_assigned\n",
    "                generation_log_probs[key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)] = generation_log_probs_temp\n",
    "                generation_perplexity[key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)] = generation_perp_temp\n",
    "                \n",
    "                p_ind+=1\n",
    "                input_ind+=1\n",
    "                \n",
    "        all_tail_ids[key+'-sampling-type_'+str(par)] = temp_tail_ids\n",
    "        all_tail_cdfs[key+'-sampling-type_'+str(par)] = temp_tail_cdfs\n",
    "        \n",
    "        print('number of prompts where the target is too short for this param', num_target_too_short)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_tail_ids, gzip.open('NewTFS_and_Flat_Tail_IDs_dict_Aug5.pickle', 'wb'))\n",
    "pickle.dump(all_tail_cdfs, gzip.open('NewTFS_and_Flat_Tail_CDFs_dict_Aug5.pickle', 'wb'))\n",
    "pickle.dump(ground_token_in_cut, gzip.open('NewTFS_and_Flat_GTruth_In_Cut_Aug5.pickle', 'wb'))\n",
    "pickle.dump(ground_truth_probs, gzip.open('NewTFS_and_Flat_Ground_Truth_Probs_For_Each_Generation_Aug5.pickle', 'wb'))\n",
    "pickle.dump(generation_log_probs, gzip.open('NewTFS_and_Flat_Generated_Token_Log_Probs_Aug5.pickle', 'wb'))\n",
    "pickle.dump(generation_perplexity, gzip.open('NewTFS_and_Flat_Generated_Perplexities_Aug5.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "all_tail_ids = pickle.load(gzip.open('NewTFS_and_Flat_Tail_IDs_dict_Aug5.pickle', 'rb'))\n",
    "all_tail_cdfs = pickle.load(gzip.open('NewTFS_and_Flat_Tail_CDFs_dict_Aug5.pickle', 'rb'))\n",
    "ground_token_in_cut=pickle.load(gzip.open('NewTFS_and_Flat_GTruth_In_Cut_Aug5.pickle', 'rb'))\n",
    "ground_truth_probs=pickle.load(gzip.open('NewTFS_and_Flat_Ground_Truth_Probs_For_Each_Generation_Aug5.pickle', 'rb'))\n",
    "generation_log_probs=pickle.load(gzip.open('NewTFS_and_Flat_Generated_Token_Log_Probs_Aug5.pickle', 'rb'))\n",
    "generation_perplexity=pickle.load(gzip.open('NewTFS_and_Flat_Generated_Perplexities_Aug5.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tail_ids.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_tail_ids['tfs-sampling-type_0.25'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and compute the perplexities and probabilities of real completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_logits\n",
    "gpt_out_path = 'gpt-2_output/'\n",
    "all_perps = pickle.load( gzip.open(gpt_out_path+'all_perplexities_perplexity_scores_for_the_dataset_Human_StoryPrompts_Completion.csv.pickle.gz', 'rb'))\n",
    "all_logits = pickle.load( gzip.open(gpt_out_path+'all_logits_perplexity_scores_for_the_dataset_Human_StoryPrompts_Completion.csv.pickle.gz', 'rb')) # needed to get the probabilities\n",
    "text = pickle.load( gzip.open(gpt_out_path+'all_text_perplexity_scores_for_the_dataset_Human_StoryPrompts_Completion.csv.pickle.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 4\n",
    "#batch_size=25\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pandas as pd    \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting probs assigned to the generated text\n",
    "real_log_probs = dict()\n",
    "for batch in range(num_batches):\n",
    "    p_ind = 0\n",
    "    for ind in range(0,all_logits[batch].shape[0]):\n",
    "        real_log_probs_temp=[]\n",
    "        for time_point in range(0,all_logits[0].shape[1]):\n",
    "            sps = softmax(all_logits[batch][ind, time_point, : ])\n",
    "            token_chosen = text[batch][ind][prompt_length+time_point]\n",
    "            '''if p_ind == 0:\n",
    "                print( decoder_text([token_chosen]) )'''\n",
    "            log_prob_here = np.log(sps[token_chosen])\n",
    "            real_log_probs_temp.append(log_prob_here)\n",
    "            \n",
    "        real_log_probs['real_text_prompt_'+str(p_ind)] = real_log_probs_temp\n",
    "        p_ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''max_vals = []\n",
    "for batch in range(num_batches):\n",
    "            #for p_ind in range(batch*batch_size, (batch*batch_size)+batch_size ):\n",
    "    for ind in range(0,all_logits[batch].shape[0]):\n",
    "\n",
    "        print(softmax(all_logits[batch][ind,0,:]).max())\n",
    "        max_vals.append(softmax(all_logits[batch][ind,0,:]).max())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting average perplexity and log probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_samples =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_plot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_plot = ['tfs_0.25','tfs_0.75', 'tfs_0.95']#['tfs_0.25','k_1', 'n_0.1']\n",
    "#['tfs_0.25', 'tfs_0.95', 'k_1', 'k_200', 'n_0.9', 'flat_0.02errorbar'] #None prints all of them\n",
    "\n",
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        print(key+'-sampling-type_'+str(par)+'_prompt')\n",
    "        vals_to_plot = []\n",
    "        for p_ind in tot_good_samples:\n",
    "            \n",
    "            try:\n",
    "                vals_to_plot.append(generation_perplexity[key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)])\n",
    "            except:\n",
    "                print('couldnt load in as not here!!!!!')\n",
    "                pass\n",
    "        vals_to_plot = np.asarray(vals_to_plot)\n",
    "        \n",
    "        #if key == \"k\" and str(par)=='1':\n",
    "        '''plt.plot(vals_to_plot.T)\n",
    "        plt.show()'''\n",
    "        \n",
    "        error_bars = vals_to_plot.mean(axis=0) -(1.96*(vals_to_plot.std(axis=0)/np.sqrt(vals_to_plot.shape[0])))\n",
    "\n",
    "        \n",
    "        if params_to_plot == None: \n",
    "            plt.errorbar(np.arange(vals_to_plot.shape[1]),vals_to_plot.mean(axis=0),error_bars, label=key+'-sampling-type_'+str(par))\n",
    "        else:\n",
    "            \n",
    "            if key+'_'+str(par) in params_to_plot:\n",
    "                print('variance of', key, 'param', str(par), 'is:', vals_to_plot.mean(axis=0).var()  )\n",
    "                plt.errorbar(np.arange(vals_to_plot.shape[1]),vals_to_plot.mean(axis=0),error_bars, label=key+'-sampling-type_'+str(par))\n",
    "\n",
    "for batch in range(num_batches):\n",
    "            #for p_ind in range(batch*batch_size, (batch*batch_size)+batch_size ):\n",
    "    #for ind in range(0,all_perps[batch].shape[0]):\n",
    "    if batch==0:\n",
    "        vals_to_plot = all_perps[batch]\n",
    "    else: \n",
    "        vals_to_plot = np.concatenate((vals_to_plot,all_perps[batch]), axis=0 )\n",
    "error_bars = vals_to_plot.mean(axis=0) -(1.96*(vals_to_plot.std(axis=0)/np.sqrt(vals_to_plot.shape[0])))\n",
    "plt.errorbar(np.arange(vals_to_plot.shape[1]),vals_to_plot.mean(axis=0),error_bars ,label='Ground Truth')\n",
    "plt.title('Perplexities')         \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model assings v low probability to the real generated word at each point. But perplexity is still similar. Is the ground word still included? Or is everything in a high state of confusion? Good similar amounts of randomness? Other models are also in states of confusion but "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_plot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_plot = ['n_0.9', 'tfs_0.95']#, 'tfs_0.95']\n",
    "#['tfs_0.25', 'tfs_0.95', 'k_1', 'k_200'] #None prints all of them\n",
    "\n",
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        print(key+'-sampling-type_'+str(par)+'_prompt')\n",
    "        vals_to_plot = []\n",
    "        for p_ind in tot_good_samples:\n",
    "            try:\n",
    "                vals_to_plot.append(generation_log_probs[key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)])\n",
    "            except:\n",
    "                pass\n",
    "                #print('couldnt load in as not here.')\n",
    "        vals_to_plot = np.asarray(vals_to_plot)\n",
    "        vals_to_plot = np.exp(vals_to_plot)\n",
    "        \n",
    "        '''if key == \"k\" and str(par)=='1':\n",
    "            plt.plot(np.exp(vals_to_plot.T))\n",
    "            plt.show()'''\n",
    "        \n",
    "        error_bars = vals_to_plot.mean(axis=0)-(1.96*(vals_to_plot.std(axis=0)/np.sqrt(vals_to_plot.shape[0])))\n",
    "\n",
    "        if params_to_plot == None: \n",
    "            plt.errorbar(np.arange(vals_to_plot.shape[1]),vals_to_plot.mean(axis=0), error_bars,label=key+'-sampling-type_'+str(par))\n",
    "        else:\n",
    "            if key+'_'+str(par) in params_to_plot:\n",
    "                print('variance of', key, 'param', str(par), 'is:', vals_to_plot.mean(axis=0).var()  )\n",
    "                plt.errorbar(np.arange(vals_to_plot.shape[1]),vals_to_plot.mean(axis=0), error_bars,label=key+'-sampling-type_'+str(par))\n",
    "                '''plt.plot(vals_to_plot.mean(axis=0),label=key+'-sampling-type_'+str(par))\n",
    "                inter = np.arange(0,vals_to_plot.shape[1],25)\n",
    "                plt.errorbar(inter,vals_to_plot.mean(axis=0)[inter], error_bars[inter],label=key+'-sampling-type_'+str(par))\n",
    "                '''\n",
    "vals_to_plot = []\n",
    "for key, values in real_log_probs.items():\n",
    "    vals_to_plot.append(values)\n",
    "vals_to_plot = np.asarray(vals_to_plot)\n",
    "#plt.errorbar(np.arange(vals_to_plot.shape[1]),vals_to_plot.mean(axis=0), error_bars,label='Ground Truth')\n",
    "                \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## investigating where the stripes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generation_log_probs[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count = 0\n",
    "\n",
    "for t in range(16,20):\n",
    "    vals = []\n",
    "    print(t)\n",
    "    for k in generation_log_probs.keys():\n",
    "        if 'tfs' in k and 'sampling-type_0.25prompt' in k:\n",
    "            found_count +=1\n",
    "            #print(np.exp(generation_log_probs[k][time_point]))\n",
    "            vals.append(np.exp(generation_log_probs[k][t]))\n",
    "    print('number found', found_count)\n",
    "    print( 'mean', np.asarray(vals).mean() )\n",
    "    print('std', np.asarray(vals).std() )\n",
    "    plt.hist(vals)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_plot.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Probs Given to ground truth real completion words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean = dict()\n",
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        print(key+'-sampling-type_'+str(par)+'_prompt')\n",
    "        vals_to_plot = []\n",
    "        for p_ind in tot_good_samples:\n",
    "            try:\n",
    "                vals_to_plot.append(ground_truth_probs[key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)])\n",
    "            except:\n",
    "                print('couldnt load in as not here.')\n",
    "                pass\n",
    "        vals_to_plot = np.asarray(vals_to_plot)\n",
    "        \n",
    "        if key == \"k\" and str(par)=='1':\n",
    "            plt.plot(vals_to_plot.T)\n",
    "            plt.show()\n",
    "            plt.plot(vals_to_plot.mean(axis=0))\n",
    "            plt.show()\n",
    "            \n",
    "        if key == \"n\" and str(par)=='0.9':\n",
    "            plt.plot(vals_to_plot.T)\n",
    "            plt.show()\n",
    "            plt.plot(vals_to_plot.mean(axis=0))\n",
    "            plt.show()\n",
    "            \n",
    "        if key == \"n\" and str(par)=='0.1':\n",
    "            plt.plot(vals_to_plot.T)\n",
    "            plt.show()\n",
    "            plt.plot(vals_to_plot.mean(axis=0))\n",
    "            plt.show()\n",
    "        \n",
    "        global_mean[key+'-sampling-type_'+str(par)+'_prompt'] = (vals_to_plot.mean(), vals_to_plot.mean()-(1.96*(vals_to_plot.std()/np.sqrt(vals_to_plot.shape[0]))))\n",
    "        print('=======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(global_mean, gzip.open('Probability_given_to_ground_truth_dict_July14.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means =[]\n",
    "errors = []\n",
    "for k, v in global_mean.items():\n",
    "    means.append(v[0])\n",
    "    errors.append(v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(global_mean.keys(), means, yerr=errors)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I could also try to get a binary output for whether or not the true word was above the tail id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_plot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean = dict()\n",
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        print(key+'-sampling-type_'+str(par)+'_prompt')\n",
    "        vals_to_plot = []\n",
    "        for p_ind in tot_good_samples:\n",
    "            try:\n",
    "                vals_to_plot.append(ground_token_in_cut[key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)])\n",
    "            except:\n",
    "                pass\n",
    "                #print('couldnt load in as not here.')\n",
    "        vals_to_plot = np.asarray(vals_to_plot)\n",
    "        \n",
    "        if key == \"k\" and str(par)=='1':\n",
    "            plt.plot(vals_to_plot.T)\n",
    "            plt.show()\n",
    "            plt.plot(vals_to_plot.mean(axis=0))\n",
    "            plt.show()\n",
    "            \n",
    "        if key == \"n\" and str(par)=='0.9':\n",
    "            plt.plot(vals_to_plot.T)\n",
    "            plt.show()\n",
    "            plt.plot(vals_to_plot.mean(axis=0))\n",
    "            plt.show()\n",
    "        \n",
    "        global_mean[key+'-sampling-type_'+str(par)+'_prompt'] = (vals_to_plot.mean(), vals_to_plot.mean()-(1.96*(vals_to_plot.std()/np.sqrt(vals_to_plot.shape[0]))))\n",
    "        print('=======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means =[]\n",
    "errors = []\n",
    "for k, v in global_mean.items():\n",
    "    means.append(v[0])\n",
    "    errors.append(v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(global_mean.keys(), means, yerr=errors)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Tail ID positions and CDFs, NB this is for different prob dists for each. \n",
    "\n",
    "Need to run for the same one\n",
    "And get binary output for if the true word is above the tail id or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''all_tail_tfs_ids = pickle.load(gzip.open('Tail_IDs_TFS_dict_July14.pickle', 'rb'))\n",
    "all_tail_tfs_cdfs = pickle.load(gzip.open('Tail_CDFs_TFS_dict_July14.pickle', 'rb'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(range(5)) - set([3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for k in ground_token_in_cut.keys():\n",
    "    li.append(int(k.split('_')[-1]))\n",
    "inside = set(li)\n",
    "missing = set(range(100)) - inside\n",
    "list(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside = list(inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to have them all compete against each other to see what is the highest in each of the positions. \n",
    "\n",
    "very_first = True\n",
    "\n",
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        if tot_num != all_tail_ids[key+'-sampling-type_'+str(par)].shape[0]:\n",
    "            print('these sizes are not the same!')\n",
    "            print('trimming the last ones')\n",
    "            all_tail_ids[key+'-sampling-type_'+str(par)] =  all_tail_ids[key+'-sampling-type_'+str(par)][inside,:]\n",
    "            all_tail_cdfs[key+'-sampling-type_'+str(par)] =  all_tail_cdfs[key+'-sampling-type_'+str(par)][inside,:]\n",
    "\n",
    "            \n",
    "        flat_ids = all_tail_ids[key+'-sampling-type_'+str(par)].flatten()\n",
    "        flat_cdfs = all_tail_cdfs[key+'-sampling-type_'+str(par)].flatten()\n",
    "\n",
    "        if very_first ==True:\n",
    "\n",
    "            tail_id_df = pd.DataFrame({key+'-sampling-type_'+str(par):flat_ids})\n",
    "            tail_cdf_df = pd.DataFrame({key+'-sampling-type_'+str(par):flat_cdfs})\n",
    "            very_first=False\n",
    "\n",
    "        else:\n",
    "\n",
    "            tail_id_df[key+'-sampling-type_'+str(par)] = flat_ids\n",
    "            tail_cdf_df[key+'-sampling-type_'+str(par)] = flat_cdfs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_cdf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_id_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_id_df.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(tail_id_df.iloc[0,:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_id_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_id_df.rank(axis=1, method='average').iloc[0].max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_id_df_ranking = tail_id_df.rank(axis=1, method='dense')  # lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_id_df_ranking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_cdf_df_ranking = tail_cdf_df.rank(axis=1, method='dense')\n",
    "tail_cdf_df_ranking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tail_id_df_ranking.columns: \n",
    "    \n",
    "    tail_id_df_ranking[c].hist(bins=100)\n",
    "    plt.title('Tail ID Ranking '+c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs_n_cols = []\n",
    "for c in tail_id_df.columns:\n",
    "    if 'tfs-' in c or 'n-' in c:\n",
    "        tfs_n_cols.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs_n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for tfs and n\n",
    "temp_rank = tail_id_df[tfs_n_cols].rank(axis=1, method='dense')\n",
    "for c in temp_rank.columns: \n",
    "\n",
    "    temp_rank[c].hist(bins=100)\n",
    "    plt.title('Tail ID Ranking '+c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_cdf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_cdf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_cdf_df['tfs-sampling-type_0.25'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tail_cdf_df['n-sampling-type_0.5']<0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tail_cdf_df.columns: \n",
    "    \n",
    "    tail_cdf_df[c].hist(bins=100)\n",
    "    plt.title(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tail_cdf_df_ranking.columns: \n",
    "    \n",
    "    tail_cdf_df_ranking[c].hist(bins=100)\n",
    "    plt.title(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the Samplings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all_tail_cdfs = pickle.load(gzip.open(\\'Tail_CDFs_dict_July14.pickle\\', \\'rb\\')) # get this from \"Analyze Logits\"\\nall_tail_ids = pickle.load(gzip.open(\\'Tail_IDs_dict_July14.pickle\\', \\'rb\\'))'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''all_tail_cdfs = pickle.load(gzip.open('Tail_CDFs_dict_July14.pickle', 'rb')) # get this from \"Analyze Logits\"\n",
    "all_tail_ids = pickle.load(gzip.open('Tail_IDs_dict_July14.pickle', 'rb'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(all_tail_cdfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tail_cdfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tail_cdfs['n-sampling-type_0.25'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_first = True\n",
    "\n",
    "vals_dict = {'n': [0.1, 0.25, 0.5, 0.75, 0.9], 'k':[1,10,40,200], 'tfs':[None, 0.01, 0.05, 0.1, 0.5, 0.75 ]}\n",
    "\n",
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        \n",
    "        specific_ids = all_tail_ids[key+'-sampling-type_'+str(par)][select_a_random_prompt,:]\n",
    "        specific_cdfs = all_tail_cdfs[key+'-sampling-type_'+str(par)][select_a_random_prompt,:]\n",
    "\n",
    "        if very_first ==True:\n",
    "\n",
    "            tail_id_df = pd.DataFrame({key+'-sampling-type_'+str(par):specific_ids})\n",
    "            tail_cdf_df = pd.DataFrame({key+'-sampling-type_'+str(par):specific_cdfs})\n",
    "            very_first=False\n",
    "\n",
    "        else:\n",
    "\n",
    "            tail_id_df[key+'-sampling-type_'+str(par)] = specific_ids\n",
    "            tail_cdf_df[key+'-sampling-type_'+str(par)] = specific_cdfs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_cdf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'tfs'\n",
    "par = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_cdf_df[tail_cdf_df[key+'-sampling-type_'+str(par)].values>0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_id_df.loc[list(tail_cdf_df[tail_cdf_df[key+'-sampling-type_'+str(par)].values>0.95].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_id_df[tail_id_df[key+'-sampling-type_'+str(par)].values>50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_logits\n",
    "\n",
    "all_logits = pickle.load( gzip.open('gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "text = pickle.load( gzip.open('gpt-2_output/all_text_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "len(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "from decodeLogits import *\n",
    "batch = select_a_random_prompt//batch_size\n",
    "ind = select_a_random_prompt - batch*batch_size\n",
    "\n",
    "all_logits[batch][ind].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pandas as pd    \n",
    "import torch\n",
    "\n",
    "#time_point = 1\n",
    "plot_window_lim =30\n",
    "\n",
    "'''def ema_eff(alpha,  vals, perc_acc=0.99 ):\n",
    "    k = int(np.log(1-perc_acc)/np.log(1-alpha)) # this should be calculated at the start not in the loop!! \n",
    "    \n",
    "    if k>vals.shape[0]: # CHECK THIS SHAPE MEASUREMENT\n",
    "        k = vals.shape[0]\n",
    "    \n",
    "    # have something to check the tail id is less than the window size. \n",
    "    \n",
    "    window_weights = (1-alpha)**np.arange(0,k)\n",
    "    p = k-1\n",
    "    # THIS CAN BE DONE IN A BATCH V EFFICIENTLY\n",
    "    out = torch.nn.functional.conv1d(torch.from_numpy(vals).unsqueeze(0).unsqueeze(1).double(),torch.from_numpy(window_weights).unsqueeze(0).unsqueeze(1), padding=p )\n",
    "    out = alpha*out[0,0,p:]\n",
    "    #print(out.shape)\n",
    "    \n",
    "    return out.numpy()'''\n",
    "\n",
    "def new_tfs(second, thresh):\n",
    "    only_pos = np.abs(second)\n",
    "    sec_indices = np.arange(len(second))\n",
    "    sec_weights = only_pos/only_pos.sum()\n",
    "    tail_id = np.argmax(np.cumsum(sec_weights)>thresh)+1\n",
    "    return tail_id\n",
    "        \n",
    "def flat(sps, p):\n",
    "    return sps.shape[0]-np.argmax(np.flip(sps)>p)\n",
    "\n",
    "print('Prompt: \\n')\n",
    "print(decoder_text( text[batch][ind, :prompt_length]))\n",
    "print('\\n ====== \\n ')\n",
    "print('Generation: \\n ')\n",
    "gen = text[batch][ind, prompt_length:]\n",
    "print(decoder_text( text[batch][ind, prompt_length:]))\n",
    "print('======')\n",
    "\n",
    "for time_point in range(0,10):\n",
    "    \n",
    "    sps = softmax(-np.sort(-all_logits[batch][ind, :, time_point]))\n",
    "    indices = np.argsort(-all_logits[batch][ind, :, time_point])\n",
    "    first = sps[1:] - sps[:-1]\n",
    "    second = first[1:] - first[:-1]\n",
    "    \n",
    "    plt.plot(np.arange(sps.shape[0]),sps)\n",
    "    \n",
    "    if key == 'tfs':\n",
    "        tail_id = new_tfs(sps, par)\n",
    "    elif key=='flat':\n",
    "        tail_id = flat(sps, par)\n",
    "    \n",
    "    \n",
    "    print('tail id is:', tail_id)\n",
    "    plt.axvline(tail_id, color='purple', linestyle='solid')\n",
    "    \n",
    "    plt.xlim([0,plot_window_lim])\n",
    "    plt.xticks(np.arange(plot_window_lim), decoder_text(indices[:plot_window_lim]).split(' ')[1:], rotation='vertical')\n",
    "    plt.title('Prob Dist')\n",
    "    plt.show()\n",
    "        \n",
    "    '''plt.plot(np.arange(second.shape[0]),second)\n",
    "    plt.xlim([0,plot_window_lim])\n",
    "    plt.axhline(0.00001, color='red', linestyle='solid')\n",
    "    plt.title('Second Gradient')\n",
    "    plt.show()'''\n",
    "    \n",
    "    if tail_id>500:\n",
    "        print('tail ID isnt specific enough, skipping this plot')\n",
    "        print('=============')\n",
    "        continue\n",
    "        \n",
    "    ids_above_tail = indices[:tail_id] # use these indices to determine what the words are and their probabilities. \n",
    "    tail_free_probs = softmax(all_logits[b, ids_above_tail, i])\n",
    "\n",
    "    plt.plot(np.arange(tail_free_probs.shape[0]), tail_free_probs)\n",
    "    plt.title('Tail Free Probs')\n",
    "    plt.xticks(np.arange(tail_free_probs.shape[0]), decoder_text(ids_above_tail).split(' ')[1:], rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "    #print('words in order',decoder_text(ids_above_tail))\n",
    "\n",
    "    print('=============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
