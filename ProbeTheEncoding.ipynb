{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-b0ced0b1968d>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b0ced0b1968d>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# enc.decode(out[i])  is where the decoding of the differnet integers happens. \n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "with open('encoder.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    def bytes_to_unicode():\n",
    "        \"\"\"\n",
    "        Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "        The reversible bpe codes work on unicode strings.\n",
    "        This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "        When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "        This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "        To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "        And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "        \"\"\"\n",
    "        bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "        cs = bs[:]\n",
    "        n = 0\n",
    "        for b in range(2**8):\n",
    "            if b not in bs:\n",
    "                bs.append(b)\n",
    "                cs.append(2**8+n)\n",
    "                n += 1\n",
    "        cs = [chr(n) for n in cs]\n",
    "        return dict(zip(bs, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = {v:k for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 33,\n",
       " '\"': 34,\n",
       " '#': 35,\n",
       " '$': 36,\n",
       " '%': 37,\n",
       " '&': 38,\n",
       " \"'\": 39,\n",
       " '(': 40,\n",
       " ')': 41,\n",
       " '*': 42,\n",
       " '+': 43,\n",
       " ',': 44,\n",
       " '-': 45,\n",
       " '.': 46,\n",
       " '/': 47,\n",
       " '0': 48,\n",
       " '1': 49,\n",
       " '2': 50,\n",
       " '3': 51,\n",
       " '4': 52,\n",
       " '5': 53,\n",
       " '6': 54,\n",
       " '7': 55,\n",
       " '8': 56,\n",
       " '9': 57,\n",
       " ':': 58,\n",
       " ';': 59,\n",
       " '<': 60,\n",
       " '=': 61,\n",
       " '>': 62,\n",
       " '?': 63,\n",
       " '@': 64,\n",
       " 'A': 65,\n",
       " 'B': 66,\n",
       " 'C': 67,\n",
       " 'D': 68,\n",
       " 'E': 69,\n",
       " 'F': 70,\n",
       " 'G': 71,\n",
       " 'H': 72,\n",
       " 'I': 73,\n",
       " 'J': 74,\n",
       " 'K': 75,\n",
       " 'L': 76,\n",
       " 'M': 77,\n",
       " 'N': 78,\n",
       " 'O': 79,\n",
       " 'P': 80,\n",
       " 'Q': 81,\n",
       " 'R': 82,\n",
       " 'S': 83,\n",
       " 'T': 84,\n",
       " 'U': 85,\n",
       " 'V': 86,\n",
       " 'W': 87,\n",
       " 'X': 88,\n",
       " 'Y': 89,\n",
       " 'Z': 90,\n",
       " '[': 91,\n",
       " '\\\\': 92,\n",
       " ']': 93,\n",
       " '^': 94,\n",
       " '_': 95,\n",
       " '`': 96,\n",
       " 'a': 97,\n",
       " 'b': 98,\n",
       " 'c': 99,\n",
       " 'd': 100,\n",
       " 'e': 101,\n",
       " 'f': 102,\n",
       " 'g': 103,\n",
       " 'h': 104,\n",
       " 'i': 105,\n",
       " 'j': 106,\n",
       " 'k': 107,\n",
       " 'l': 108,\n",
       " 'm': 109,\n",
       " 'n': 110,\n",
       " 'o': 111,\n",
       " 'p': 112,\n",
       " 'q': 113,\n",
       " 'r': 114,\n",
       " 's': 115,\n",
       " 't': 116,\n",
       " 'u': 117,\n",
       " 'v': 118,\n",
       " 'w': 119,\n",
       " 'x': 120,\n",
       " 'y': 121,\n",
       " 'z': 122,\n",
       " '{': 123,\n",
       " '|': 124,\n",
       " '}': 125,\n",
       " '~': 126,\n",
       " '¡': 161,\n",
       " '¢': 162,\n",
       " '£': 163,\n",
       " '¤': 164,\n",
       " '¥': 165,\n",
       " '¦': 166,\n",
       " '§': 167,\n",
       " '¨': 168,\n",
       " '©': 169,\n",
       " 'ª': 170,\n",
       " '«': 171,\n",
       " '¬': 172,\n",
       " '®': 174,\n",
       " '¯': 175,\n",
       " '°': 176,\n",
       " '±': 177,\n",
       " '²': 178,\n",
       " '³': 179,\n",
       " '´': 180,\n",
       " 'µ': 181,\n",
       " '¶': 182,\n",
       " '·': 183,\n",
       " '¸': 184,\n",
       " '¹': 185,\n",
       " 'º': 186,\n",
       " '»': 187,\n",
       " '¼': 188,\n",
       " '½': 189,\n",
       " '¾': 190,\n",
       " '¿': 191,\n",
       " 'À': 192,\n",
       " 'Á': 193,\n",
       " 'Â': 194,\n",
       " 'Ã': 195,\n",
       " 'Ä': 196,\n",
       " 'Å': 197,\n",
       " 'Æ': 198,\n",
       " 'Ç': 199,\n",
       " 'È': 200,\n",
       " 'É': 201,\n",
       " 'Ê': 202,\n",
       " 'Ë': 203,\n",
       " 'Ì': 204,\n",
       " 'Í': 205,\n",
       " 'Î': 206,\n",
       " 'Ï': 207,\n",
       " 'Ð': 208,\n",
       " 'Ñ': 209,\n",
       " 'Ò': 210,\n",
       " 'Ó': 211,\n",
       " 'Ô': 212,\n",
       " 'Õ': 213,\n",
       " 'Ö': 214,\n",
       " '×': 215,\n",
       " 'Ø': 216,\n",
       " 'Ù': 217,\n",
       " 'Ú': 218,\n",
       " 'Û': 219,\n",
       " 'Ü': 220,\n",
       " 'Ý': 221,\n",
       " 'Þ': 222,\n",
       " 'ß': 223,\n",
       " 'à': 224,\n",
       " 'á': 225,\n",
       " 'â': 226,\n",
       " 'ã': 227,\n",
       " 'ä': 228,\n",
       " 'å': 229,\n",
       " 'æ': 230,\n",
       " 'ç': 231,\n",
       " 'è': 232,\n",
       " 'é': 233,\n",
       " 'ê': 234,\n",
       " 'ë': 235,\n",
       " 'ì': 236,\n",
       " 'í': 237,\n",
       " 'î': 238,\n",
       " 'ï': 239,\n",
       " 'ð': 240,\n",
       " 'ñ': 241,\n",
       " 'ò': 242,\n",
       " 'ó': 243,\n",
       " 'ô': 244,\n",
       " 'õ': 245,\n",
       " 'ö': 246,\n",
       " '÷': 247,\n",
       " 'ø': 248,\n",
       " 'ù': 249,\n",
       " 'ú': 250,\n",
       " 'û': 251,\n",
       " 'ü': 252,\n",
       " 'ý': 253,\n",
       " 'þ': 254,\n",
       " 'ÿ': 255,\n",
       " 'Ā': 0,\n",
       " 'ā': 1,\n",
       " 'Ă': 2,\n",
       " 'ă': 3,\n",
       " 'Ą': 4,\n",
       " 'ą': 5,\n",
       " 'Ć': 6,\n",
       " 'ć': 7,\n",
       " 'Ĉ': 8,\n",
       " 'ĉ': 9,\n",
       " 'Ċ': 10,\n",
       " 'ċ': 11,\n",
       " 'Č': 12,\n",
       " 'č': 13,\n",
       " 'Ď': 14,\n",
       " 'ď': 15,\n",
       " 'Đ': 16,\n",
       " 'đ': 17,\n",
       " 'Ē': 18,\n",
       " 'ē': 19,\n",
       " 'Ĕ': 20,\n",
       " 'ĕ': 21,\n",
       " 'Ė': 22,\n",
       " 'ė': 23,\n",
       " 'Ę': 24,\n",
       " 'ę': 25,\n",
       " 'Ě': 26,\n",
       " 'ě': 27,\n",
       " 'Ĝ': 28,\n",
       " 'ĝ': 29,\n",
       " 'Ğ': 30,\n",
       " 'ğ': 31,\n",
       " 'Ġ': 32,\n",
       " 'ġ': 127,\n",
       " 'Ģ': 128,\n",
       " 'ģ': 129,\n",
       " 'Ĥ': 130,\n",
       " 'ĥ': 131,\n",
       " 'Ħ': 132,\n",
       " 'ħ': 133,\n",
       " 'Ĩ': 134,\n",
       " 'ĩ': 135,\n",
       " 'Ī': 136,\n",
       " 'ī': 137,\n",
       " 'Ĭ': 138,\n",
       " 'ĭ': 139,\n",
       " 'Į': 140,\n",
       " 'į': 141,\n",
       " 'İ': 142,\n",
       " 'ı': 143,\n",
       " 'Ĳ': 144,\n",
       " 'ĳ': 145,\n",
       " 'Ĵ': 146,\n",
       " 'ĵ': 147,\n",
       " 'Ķ': 148,\n",
       " 'ķ': 149,\n",
       " 'ĸ': 150,\n",
       " 'Ĺ': 151,\n",
       " 'ĺ': 152,\n",
       " 'Ļ': 153,\n",
       " 'ļ': 154,\n",
       " 'Ľ': 155,\n",
       " 'ľ': 156,\n",
       " 'Ŀ': 157,\n",
       " 'ŀ': 158,\n",
       " 'Ł': 159,\n",
       " 'ł': 160,\n",
       " 'Ń': 173}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_encoder = bytes_to_unicode()\n",
    "byte_decoder = {v:k for k, v in byte_encoder.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "import gzip\n",
    "all_logits = pickle.load(gzip.open('gpt-2_output/all_logits5000_word_prompts.pickle.gz', 'rb'))\n",
    "rand_selection = pickle.load(gzip.open('gpt-2_output/prompt_rand_selections_5000_word_prompts.pickle.gz', 'rb'))\n",
    "df=pd.read_csv('test_dataframe_200primer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_source</th>\n",
       "      <th>test_target</th>\n",
       "      <th>test_target_200</th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>[WP] You look out at the bright lights of the ...</td>\n",
       "      <td>The lights dance across the evening sky. You c...</td>\n",
       "      <td>The lights dance across the evening sky. You c...</td>\n",
       "      <td>[WP] You look out at the bright lights of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3768</th>\n",
       "      <td>[FF] - 250 Words; 2 months Reddit Gold prize</td>\n",
       "      <td>Could this be it? I could have sworn, no... no...</td>\n",
       "      <td>Could this be it? I could have sworn, no... no...</td>\n",
       "      <td>[FF] - 250 Words; 2 months Reddit Gold prize \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912</th>\n",
       "      <td>[WP] The shortest story ever told. Once upon a...</td>\n",
       "      <td>Once upon a time, there was a magical place wh...</td>\n",
       "      <td>Once upon a time, there was a magical place wh...</td>\n",
       "      <td>[WP] The shortest story ever told. Once upon a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14879</th>\n",
       "      <td>[WP] Stockholm syndrome, but the other way aro...</td>\n",
       "      <td>``Here it is. 2 million dollars in unmarked bi...</td>\n",
       "      <td>``Here it is. 2 million dollars in unmarked bi...</td>\n",
       "      <td>[WP] Stockholm syndrome, but the other way aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4848</th>\n",
       "      <td>[WP] Your grandparents always playfully bicker...</td>\n",
       "      <td>It wasn ’ t Swedish. Well I have no way of act...</td>\n",
       "      <td>It wasn ’ t Swedish. Well I have no way of act...</td>\n",
       "      <td>[WP] Your grandparents always playfully bicker...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             test_source  \\\n",
       "5139   [WP] You look out at the bright lights of the ...   \n",
       "3768        [FF] - 250 Words; 2 months Reddit Gold prize   \n",
       "3912   [WP] The shortest story ever told. Once upon a...   \n",
       "14879  [WP] Stockholm syndrome, but the other way aro...   \n",
       "4848   [WP] Your grandparents always playfully bicker...   \n",
       "\n",
       "                                             test_target  \\\n",
       "5139   The lights dance across the evening sky. You c...   \n",
       "3768   Could this be it? I could have sworn, no... no...   \n",
       "3912   Once upon a time, there was a magical place wh...   \n",
       "14879  ``Here it is. 2 million dollars in unmarked bi...   \n",
       "4848   It wasn ’ t Swedish. Well I have no way of act...   \n",
       "\n",
       "                                         test_target_200  \\\n",
       "5139   The lights dance across the evening sky. You c...   \n",
       "3768   Could this be it? I could have sworn, no... no...   \n",
       "3912   Once upon a time, there was a magical place wh...   \n",
       "14879  ``Here it is. 2 million dollars in unmarked bi...   \n",
       "4848   It wasn ’ t Swedish. Well I have no way of act...   \n",
       "\n",
       "                                                  Prompt  \n",
       "5139   [WP] You look out at the bright lights of the ...  \n",
       "3768   [FF] - 250 Words; 2 months Reddit Gold prize \\...  \n",
       "3912   [WP] The shortest story ever told. Once upon a...  \n",
       "14879  [WP] Stockholm syndrome, but the other way aro...  \n",
       "4848   [WP] Your grandparents always playfully bicker...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[rand_selection] # checking that the random selection performed the way it was supposed to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' you ever feel like that? \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 0\n",
    "tokens = []\n",
    "for j in range( all_logits[0].shape[2]):\n",
    "    tokens.append(np.argmax(all_logits[0][ind, :, j]))\n",
    "errors='replace'\n",
    "text = ''.join([decoder[token] for token in tokens])\n",
    "text = bytearray([byte_decoder[c] for c in text]).decode('utf-8', errors=errors)\n",
    "text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġyou|Ġever|Ġfeel|Ġlike|Ġthat|?|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ|Ġ|Ċ'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join([decoder[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I use the detokenized version then I should be good. you know i can use the moses encoder on the predicted text. and then compare the originals? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation where ground truth is known: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 286,  428, 1295,   11,  475,  356, 1183, 1011, 1337,  286,  340,\n",
       "         13,  921, 1183, 1560,  262, 1644,  345, 1239, 2982,  286,  428,\n",
       "       1295,   11,  475,  356, 1183, 1011, 1337,  286,  340,   13,  921,\n",
       "       1183, 1560,  262, 1644,  345, 1239, 2982,  286,  428, 1295,   11,\n",
       "        475,  356, 1183, 1011, 1337,  286,  340,   13,  921, 1183, 1560,\n",
       "        262, 1644,  345, 1239, 2982,  286,  428, 1295,   11,  475,  356,\n",
       "       1183, 1011, 1337,  286,  340,   13,  921, 1183, 1560,  262, 1644,\n",
       "        345, 1239, 2982,  286,  428, 1295,   11,  475,  356, 1183, 1011,\n",
       "       1337,  286,  340,   13,  921, 1183, 1560,  262, 1644,  345, 1239,\n",
       "       2982])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = [ 286,  428, 1295,   11,  475,  356, 1183, 1011, 1337,  286,  340,   13,  921, 1183,\n",
    " 1560,  262, 1644,  345, 1239, 2982,  286,  428, 1295,   11,  475,  356, 1183, 1011,\n",
    " 1337,  286,  340,   13,  921, 1183, 1560,  262, 1644,  345, 1239, 2982,  286,  428,\n",
    " 1295,   11,  475,  356, 1183, 1011, 1337,  286,  340,   13,  921, 1183, 1560,  262,\n",
    " 1644,  345, 1239, 2982,  286,  428, 1295,   11,  475,  356, 1183, 1011, 1337,  286,\n",
    "  340,   13,  921, 1183, 1560,  262, 1644,  345, 1239, 2982,  286,  428, 1295,   11,\n",
    "  475,  356, 1183, 1011, 1337,  286,  340,   13,  921, 1183, 1560,  262, 1644,  345,\n",
    " 1239, 2982]\n",
    "\n",
    "\n",
    "li = np.asarray(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" of this place, but we'll take care of it. You'll tell the police you never heard of this place, but we'll take care of it. You'll tell the police you never heard of this place, but we'll take care of it. You'll tell the police you never heard of this place, but we'll take care of it. You'll tell the police you never heard of this place, but we'll take care of it. You'll tell the police you never heard\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors='replace'\n",
    "text = ''.join([decoder[token] for token in li])\n",
    "text = bytearray([byte_decoder[c] for c in text]).decode('utf-8', errors=errors)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index output pre and post decoding: \n",
    "\n",
    "\n",
    "\n",
    "[ 286  428 1295   11  475  356 1183 1011 1337  286  340   13  921 1183\n",
    " 1560  262 1644  345 1239 2982  286  428 1295   11  475  356 1183 1011\n",
    " 1337  286  340   13  921 1183 1560  262 1644  345 1239 2982  286  428\n",
    " 1295   11  475  356 1183 1011 1337  286  340   13  921 1183 1560  262\n",
    " 1644  345 1239 2982  286  428 1295   11  475  356 1183 1011 1337  286\n",
    "  340   13  921 1183 1560  262 1644  345 1239 2982  286  428 1295   11\n",
    "  475  356 1183 1011 1337  286  340   13  921 1183 1560  262 1644  345\n",
    " 1239 2982]\n",
    "======================================== SAMPLE 1 ========================================\n",
    " of this place, but we'll take care of it. You'll tell the police you never heard of this place, but we'll take care of it. You'll tell the police you never heard of this place, but we'll take care of it. You'll tell the police you never heard of this place, but we'll take care of it. You'll tell the police you never heard of this place, but we'll take care of it. You'll tell the police you never heard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
