{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the difference between nucleus and TFS set of words. See if the words in this set are reasonable. They should be reasonable for when TFS is looser and should not be when TFS is tighter. Out of this set of words, take the highest, lowest prob ones and the middle one? Need at least 3 word gap between them. \n",
    "Rank each one in how reasonable it is. Control baseline is how reasonable words at different points in the body are. The first word, half way and at the closer of the tails. Can also get words outside of the last tail, just outside, halfway to the end, and at the very end. \n",
    "There are 9 different words in total. Should i rank all of them, scramble up a subset? Or have a ranking for each of the subsets differently? Probably scramble them all up. And then present 3 of them??\n",
    "Can then see overall how replaceable different parts of the distributions is, and for TFS and nucleus specifically. Have a control with the bad words. And should be able to hopefully show that the difference in replaceability between the TFS and nucleus words is small when TFS is looser and large when TFS is tighter. Can see how specific this is also. Eg could just look at the last TFS and nucleus words, see the diff in replaceability, or maybe need to look at the averages. Look most at this unique subset and these three points should hopefully categorize it. Want them because i dont know how good the signal is going to be. For the closest words for example. \n",
    "Can also get overall data on how replaceability corresponds to model probability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pandas as pd    \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decodeLogits import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" {'tfs':[0.25, 0.75, 0.9, 0.95, 0.99], 'flat':[0.01, 0.02, 0.05],\\n'n': [0.1, 0.25, 0.5, 0.63, 0.69, 0.75, 0.81, 0.9], 'k':[1,10,40,200]  }\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#updated\n",
    "vals_dict = {'tfs':[0.25, 0.75, 0.9, 0.95, 0.99],\n",
    "'n': [0.5, 0.63, 0.69, 0.81, 0.75, 0.9], 'k':[1,40,200]  }\n",
    "\n",
    "\n",
    "''' {'tfs':[0.25, 0.75, 0.9, 0.95, 0.99], 'flat':[0.01, 0.02, 0.05],\n",
    "'n': [0.1, 0.25, 0.5, 0.63, 0.69, 0.75, 0.81, 0.9], 'k':[1,10,40,200]  }'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing analysis for the same logit going to use the ground truth as it is a Schelling point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "\n",
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=25\n",
    "num_batches=4\n",
    "prompt_length=100\n",
    "generated_length=150\n",
    "tot_len = prompt_length+generated_length\n",
    "\n",
    "import encoder\n",
    "model_name='345M'\n",
    "models_dir='../gpt-2/models'\n",
    "enc = encoder.get_encoder(model_name, models_dir)\n",
    "\n",
    "prompts=pd.read_csv('test_dataframe_500primer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_out_path = 'gpt-2_output/'\n",
    "additional_path = '-model_774M-seed_27'\n",
    "#all_perps = pickle.load( gzip.open(gpt_out_path+'all_perplexities_perplexity_scores_for_the_dataset_Human_StoryPrompts_Completion.csv'+additional_path+'.pickle.gz', 'rb'))\n",
    "all_logits = pickle.load( gzip.open(gpt_out_path+'all_logits_perplexity_scores_for_the_dataset_Human_StoryPrompts_Completion.csv'+additional_path+'.pickle.gz', 'rb')) # needed to get the probabilities\n",
    "text = pickle.load( gzip.open(gpt_out_path+'all_text_perplexity_scores_for_the_dataset_Human_StoryPrompts_Completion.csv'+additional_path+'.pickle.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "24\n",
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_num = 0\n",
    "mapping_to_batch = dict()\n",
    "for i in range(num_batches):\n",
    "    num_in_batch = all_logits[i].shape[0]\n",
    "    for el, b_ind in zip(range(tot_num, tot_num+num_in_batch), range(0, num_in_batch)):\n",
    "        mapping_to_batch[el] = (i, b_ind) # actual batch and then the ind in that batch\n",
    "    print( num_in_batch)\n",
    "    tot_num+= num_in_batch\n",
    "tot_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 150, 50257)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to also focus on the maximum deviation portions at some point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random prompts and locations within the 150 generation locations\n",
    "num_prompts_and_timepoints_wanted = 10\n",
    "leading_prompt = 15\n",
    "\n",
    "rand_prompts = np.random.randint(0,tot_num, num_prompts_and_timepoints_wanted)\n",
    "# leadingprompt until 150. as the logits are 150 not 250. \n",
    "# -1 because of the end token prediction\n",
    "rand_times = np.random.randint(0,generated_length-1, num_prompts_and_timepoints_wanted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting randomly from the max deviations: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "\n",
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_logits[0][0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = [1,2,3,4,5]\n",
    "lol[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting random locations\n",
    "for the differnet gen strategies. find the tail locations. take the tokens at each position. \n",
    "move onto the next random location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough space between the tail locations\n",
      "not enough space between the tail locations\n",
      "not enough space between the tail locations\n",
      "not enough space between the tail locations\n",
      "not enough space between the tail locations\n"
     ]
    }
   ],
   "source": [
    "# applying all analyses to the logits. \n",
    "prompt_length = 100\n",
    "methods_wanted = ['tfs_0.95', 'n_0.69']#, 'k_40']\n",
    "\n",
    "to_df = []\n",
    "\n",
    "# randomly select generations and positions in those generations. from the original prompts. \n",
    "for r_ind in range(0, num_prompts_and_timepoints_wanted):\n",
    "    \n",
    "    prompt = rand_prompts[r_ind]\n",
    "    time_point = rand_times[r_ind]\n",
    "    text_timepoint = time_point+prompt_length+1 # +1 because the perplexities keeps the end prediction token. \n",
    "    batch_ind, ind_in_batch = mapping_to_batch[prompt]\n",
    "    \n",
    "    diff_tail_positions = dict()\n",
    "    largest_tail_id = 0\n",
    "    method_of_largest_tail_id = None\n",
    "    \n",
    "    sps = softmax(-np.sort(-all_logits[batch_ind][ind_in_batch, time_point, :]))\n",
    "    indices = np.argsort(-all_logits[batch_ind][ind_in_batch, time_point, :])\n",
    "    ground_token = text[batch_ind][ind_in_batch][text_timepoint]\n",
    "                \n",
    "    for key,all_params in vals_dict.items():\n",
    "        for par in all_params:\n",
    "            method = key+'_'+str(par)\n",
    "            if method in methods_wanted:\n",
    "            \n",
    "                if key == 'tfs':\n",
    "                    first = sps[1:] - sps[:-1]\n",
    "                    second = first[1:] - first[:-1]\n",
    "                    tail_id = new_tfs(second, par)\n",
    "                elif key=='flat':\n",
    "                    tail_id = flat(sps, par)\n",
    "                elif key=='n':\n",
    "                    tail_id = nucleus_calc(sps, par)\n",
    "                elif key=='k':\n",
    "                    tail_id = par\n",
    "                else:\n",
    "                    print('key not recognized')\n",
    "                    break\n",
    "                    \n",
    "                if tail_id > largest_tail_id:\n",
    "                    largest_tail_id=tail_id\n",
    "                    method_of_largest_tail_id=method\n",
    "            \n",
    "                diff_tail_positions[method] = tail_id#, ids_above_tail)\n",
    "\n",
    "    if len(diff_tail_positions.keys()) >2: \n",
    "        print('CURRENTLY CANT LOOK AT MORE THAN 2 THINGS!')\n",
    "        break\n",
    "        \n",
    "    # indices is still sorted. and can be used \n",
    "    smaller_method = list(diff_tail_positions.keys())\n",
    "    smaller_method.remove(method_of_largest_tail_id)\n",
    "    smaller_method = smaller_method[0]\n",
    "    smaller_tail_id = diff_tail_positions[smaller_method]\n",
    "    \n",
    "    if largest_tail_id - smaller_tail_id < 3:\n",
    "        print('not enough space between the tail locations')\n",
    "        continue\n",
    "        \n",
    "    # what the ground token is and what the looser method is here. \n",
    "    ground_word = decoder_text([ground_token])\n",
    "    \n",
    "    #get the 15 words in front. \n",
    "    lead_prompt_words = decoder_text(text[batch_ind][ind_in_batch][text_timepoint-leading_prompt:text_timepoint])\n",
    "    df_row = [prompt, time_point, batch_ind, ind_in_batch, lead_prompt_words, ground_word, method_of_largest_tail_id] # taking it from the last one. \n",
    "    \n",
    "    for method in methods_wanted: # want to do them in the same order.\n",
    "        # add the tail for each method: \n",
    "        df_row += [diff_tail_positions[method]]\n",
    "        if method == method_of_largest_tail_id:\n",
    "            unique_tokens = indices[smaller_tail_id:largest_tail_id]\n",
    "            starting_pos = smaller_tail_id\n",
    "        else:\n",
    "            unique_tokens = indices[:smaller_tail_id]\n",
    "            starting_pos = 0\n",
    "            \n",
    "        #remove any non word tokens: \n",
    "        unique_tokens = remove_non_words(unique_tokens)\n",
    "        # select the right positions\n",
    "        words_and_positions_wanted = get_specific_positions(unique_tokens, starting_pos)\n",
    "        \n",
    "        df_row += [words_and_positions_wanted[0], words_and_positions_wanted[1] ]\n",
    "        \n",
    "    to_df.append(df_row)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HEY'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hey'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text[0][0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(to_df, columns = ['prompt_ind', 'time_point', \n",
    "                               'batch_ind', 'ind_in_batch', 'leading_words',\n",
    "                               'ground_word', 'looser_method', 'tail_pos_m1', \n",
    "        'specific_words_m1', 'positions_m1','tail_pos_m2','specific_words_m2', 'positions_m2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_ind</th>\n",
       "      <th>time_point</th>\n",
       "      <th>batch_ind</th>\n",
       "      <th>ind_in_batch</th>\n",
       "      <th>leading_words</th>\n",
       "      <th>ground_word</th>\n",
       "      <th>looser_method</th>\n",
       "      <th>tail_pos_m1</th>\n",
       "      <th>specific_words_m1</th>\n",
       "      <th>positions_m1</th>\n",
       "      <th>tail_pos_m2</th>\n",
       "      <th>specific_words_m2</th>\n",
       "      <th>positions_m2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>hands together.`` My name is 28w35, this is m...</td>\n",
       "      <td>.''</td>\n",
       "      <td>tfs_0.95</td>\n",
       "      <td>7</td>\n",
       "      <td>[None, None, None]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>4</td>\n",
       "      <td>[None, None, None]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n onto Dirt, and still be mad \\n at your doo...</td>\n",
       "      <td>you</td>\n",
       "      <td>tfs_0.95</td>\n",
       "      <td>8</td>\n",
       "      <td>[your, it, they]</td>\n",
       "      <td>[3, 5, 8]</td>\n",
       "      <td>3</td>\n",
       "      <td>[you, Dirt, she]</td>\n",
       "      <td>[0, 1, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>uses of technology. The infantry, what Hitosh...</td>\n",
       "      <td>to</td>\n",
       "      <td>n_0.69</td>\n",
       "      <td>32</td>\n",
       "      <td>[the, now, mainly]</td>\n",
       "      <td>[0, 16, 32]</td>\n",
       "      <td>111</td>\n",
       "      <td>[heavily, under, expected]</td>\n",
       "      <td>[32, 70, 109]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>a bang, a bit like a firecracker, God appeare...</td>\n",
       "      <td>front</td>\n",
       "      <td>n_0.69</td>\n",
       "      <td>28</td>\n",
       "      <td>[doorway, car, background]</td>\n",
       "      <td>[0, 14, 28]</td>\n",
       "      <td>63</td>\n",
       "      <td>[street, midst, top]</td>\n",
       "      <td>[28, 45, 63]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>She was gorgeous to him and always had been. ...</td>\n",
       "      <td>believe</td>\n",
       "      <td>tfs_0.95</td>\n",
       "      <td>27</td>\n",
       "      <td>[see, describe, even]</td>\n",
       "      <td>[17, 22, 27]</td>\n",
       "      <td>17</td>\n",
       "      <td>[believe, stop, find]</td>\n",
       "      <td>[0, 8, 17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_ind  time_point  batch_ind  ind_in_batch  \\\n",
       "0          67         127          2            21   \n",
       "1          70          16          3             0   \n",
       "2          33         127          1            10   \n",
       "3          21          66          0            21   \n",
       "4          17          97          0            17   \n",
       "\n",
       "                                       leading_words ground_word  \\\n",
       "0   hands together.`` My name is 28w35, this is m...         .''   \n",
       "1   \\n onto Dirt, and still be mad \\n at your doo...         you   \n",
       "2   uses of technology. The infantry, what Hitosh...          to   \n",
       "3   a bang, a bit like a firecracker, God appeare...       front   \n",
       "4   She was gorgeous to him and always had been. ...     believe   \n",
       "\n",
       "  looser_method  tail_pos_m1           specific_words_m1  positions_m1  \\\n",
       "0      tfs_0.95            7          [None, None, None]     [0, 0, 0]   \n",
       "1      tfs_0.95            8            [your, it, they]     [3, 5, 8]   \n",
       "2        n_0.69           32          [the, now, mainly]   [0, 16, 32]   \n",
       "3        n_0.69           28  [doorway, car, background]   [0, 14, 28]   \n",
       "4      tfs_0.95           27       [see, describe, even]  [17, 22, 27]   \n",
       "\n",
       "   tail_pos_m2           specific_words_m2   positions_m2  \n",
       "0            4          [None, None, None]      [0, 0, 0]  \n",
       "1            3            [you, Dirt, she]      [0, 1, 3]  \n",
       "2          111  [heavily, under, expected]  [32, 70, 109]  \n",
       "3           63        [street, midst, top]   [28, 45, 63]  \n",
       "4           17       [believe, stop, find]     [0, 8, 17]  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a bang, a bit like a firecracker, God appeared in the'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.iloc[3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up =='A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "seed = 27\n",
    "prompt_length = 100\n",
    "gs=dict()\n",
    "methods_wanted = ['tfs_0.95', 'n_0.69']#, 'k_40']\n",
    "additional_path = '774M_model'\n",
    "first = True\n",
    "\n",
    "\n",
    "# randomly select generations and positions in those generations. from the original prompts. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for key, params in vals_dict.items():\n",
    "    for par in params:\n",
    "        if key+'_'+str(par) in methods_wanted:\n",
    "            # loading in all the methods\n",
    "            all_logits = pickle.load( gzip.open('gpt-2_output/all_logits_'+key+'-sampling-type_'+par+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "            text = pickle.load( gzip.open('gpt-2_output/all_text_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts_'+str(seed)+'-seed_'+additional_path+'.pickle.gz', 'rb'))\n",
    "            rand_selects = pickle.load( gzip.open('gpt-2_output/prompt_rand_selections_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts_'+str(seed)+'-seed_'+additional_path+'.pickle.gz', 'rb'))\n",
    "\n",
    "            # finding the tail locations. \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            generated_output = []\n",
    "            for batch in text:\n",
    "                for entry in batch:\n",
    "                    generated_output.append(decoder_text(entry[prompt_length:]))\n",
    "            col_name = 'Samp Type:'+key+' Param:'+str(par)\n",
    "            gs[col_name] = generated_output\n",
    "            \n",
    "    if first==True:\n",
    "        prompts_used = []\n",
    "        for batch in text:\n",
    "            for entry in batch:\n",
    "                prompts_used.append(decoder_text(entry[0:prompt_length]))\n",
    "        \n",
    "        first=False\n",
    "gs = pd.DataFrame(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Samp Type:tfs Param:0.95</th>\n",
       "      <th>Samp Type:n Param:0.69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pass, and your time is up. You are one of the...</td>\n",
       "      <td>, days, weeks, months, years. The life you liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n't I make sure he doesn't come back? \\n \\n \\n...</td>\n",
       "      <td>I try to tell it not to worry? \\n \\n \\n This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a lamp. So he was forced to create one. \\n \\n...</td>\n",
       "      <td>himself. He wished for the boy's family.\\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we have her. She's now with us.''\\n ``Oh, it'...</td>\n",
       "      <td>are holding her hostage. We'll have her for r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, was a complete fucking idiot, who could neve...</td>\n",
       "      <td>, didn ’ t quite fit the character, and didn ’...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Samp Type:tfs Param:0.95  \\\n",
       "0   pass, and your time is up. You are one of the...   \n",
       "1  n't I make sure he doesn't come back? \\n \\n \\n...   \n",
       "2   a lamp. So he was forced to create one. \\n \\n...   \n",
       "3   we have her. She's now with us.''\\n ``Oh, it'...   \n",
       "4  , was a complete fucking idiot, who could neve...   \n",
       "\n",
       "                              Samp Type:n Param:0.69  \n",
       "0  , days, weeks, months, years. The life you liv...  \n",
       "1   I try to tell it not to worry? \\n \\n \\n This ...  \n",
       "2   himself. He wished for the boy's family.\\nAnd...  \n",
       "3   are holding her hostage. We'll have her for r...  \n",
       "4  , didn ’ t quite fit the character, and didn ’...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pass, and your time is up. You are one of the last people to leave the park, and the last one to see the city. You take your last breath, and watch as the night closes in. \\n You are in your house, and your room is dark. The lights are on, and your computer is on, but you are not even sure what time it is. You sit up, and see the light from the front door come on, and your heart leaps. \\n You close your eyes as you hear the phone ring. You hear the voicemail. You pick it up, and the caller says \"Hi, I\\'m your brother. \\xa0I\\'m home. \\xa0I\\'m sorry for the long wait'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Samp Type:tfs Param:0.95     we have her. She's now with us.''\\n ``Oh, it'...\n",
       "Samp Type:n Param:0.69       are holding her hostage. We'll have her for r...\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "for ind, p in enumerate(prompts_used):\n",
    "    if 'Stockholm syndrome, but the other way around' in p:\n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      " so she can't even tell you her first name. It's like she's a mother's worst nightmare.\n",
      "How to get rid of a mother's greatest asset? Step on a crack, break a mother's back  #proud to be a mother\n",
      "You can get a new one, too.\n",
      "When women say they're a \"whore\" they mean that they have no self-control. What they mean is that they're all about sex and they'll do anything to get it. But the women who say they're a whore are always talking about one or two things:\n",
      "A) To get off\n",
      "B) To make sure they're being watched\n",
      "C) To have an orgasm\n",
      "When you see a whore\n",
      " where she's constantly exhausted and never really has much time to sleep. A single tear in a mother's eyes is worth a lifetime of misery.\n",
      "Posted by Mark Liberman at 10:26 AM<|endoftext|>It is the year 2049.\n",
      "\n",
      "As the United States military and the CIA attempt to create a \"New World Order,\" a terrorist organization known as the Valkyries is leading the charge. With the help of a mysterious woman, the Valkyries are leading a mysterious rebellion against the New World Order.\n",
      "\n",
      "In order to combat the Valkyries, the government must work with the government-in-exile, a group of rebels who have been forced to live in the shadows. The two sides must work together to battle\n",
      "39\n",
      " standing over the dinner table, he was just eating the food out of the bucket. The boy immediately starts coughing and the food gets knocked off the table, his mind not knowing what just happened, he didn't think to just throw the food away. The head chef is up and about as well, as is the old man, as he had been holding back the massive dish of food. \n",
      "``Damn this sucks. STEVE LET THE CHEESE GET THROUGH. NO MEAL AND A CHEESE CUP IN THE ROOF. I'M GETTING ANGERED AT THE SAME TIME. ``\n",
      "The two men start fighting. The young boy then reaches out to grab the head chef's shirt, which sends\n",
      " that just appeared out of nowhere and then proceeded to stab the young boy with the potatoes.\n",
      "``WHAT THE FUCK ARE YOU DOING I THOUGHT YOU WERE A FRIEND!''\n",
      "``This is what happens when you drink from the Fountain of Youth,'' said the young boy as he prepared to eat his dinner.\n",
      "``So how did this happen?'' asked the old man as the boy ran off with the bucket of potatoes.\n",
      "``Well I got lucky I guess. I was walking around with the carrot in my hand and then I ran into this guy. I was just walking around and I was walking by a shop and he just yelled out a curse word. I just grabbed the carrot and he was going to eat\n",
      "55\n",
      " I celebrated the third birthday of the human mind. It was a natural time for me to reach that milestone. All of my years of post-humanity have been spent in a single epoch of time. The human mind is a single conscious being that is both personal and universal. We've managed to come a long way from being mere ants to beings capable of thinking about the very nature of existence. A hundred years ago, a scientist at MIT named L. David Mech would have told you that our intellect is our greatest advantage. That's no longer true. We now have the power to transcend the bounds of our body. We can inhabit any part of our bodies and mind, and we have the ability to adapt to anything we encounter. It's\n",
      " I celebrated the one that I will die on. I had hoped to have made a new beginning, but I've come to realize that this life is a far cry from the one I've chosen, one I hoped to experience as a child. I'm only now beginning to understand the meaning of death. For me, death is not the end, but its beginning. In the same way, life is not the end of something but the beginning. The last few months have been a journey into this dark labyrinth that is the afterlife, and I can't even begin to begin to describe it. The only thing I can say is that, if I don't end up in hell, I don't want to die in hell.\n",
      "I'm not\n",
      "97\n",
      " up the blood and vomit of my dead Santa. A cold wind blows through the living room as the red carpet flutters in the wind.\n",
      "I put the blood stain and the vomit in the dishwasher and run it through the washer. I pour off the hot water and put the dishes in the sink. I dry the dishes, and dry the carpet and then put the carpet back in. The blood stains and vomit on the carpet were so thick that I could barely see them. I went to bed and took a shower.\n",
      "I woke up the next morning, and noticed that the blood stains were still there. I washed them off. I put on a red sweater and put on a pair of gloves. I went outside the door of\n",
      " it off of the carpet. ''Nuh uh.. we got ta get a new carpet for the house.. we'll get one for the office in the basement. My wife turns to me and says `We don't need that.'' So I stand up and ask my wife to go to the kitchen. My wife turns around and looks at me. She says `Dad, you don't need to do that.' `No, we don't need to do that.' She turns to her husband and says `You don't need to do that.' `No, we don't need to do that.' I turn to my wife and say `You know, Mom, I can't be responsible for what my kids do.` I turn to my\n",
      "96\n",
      " started going out, I found myself very close to being able to \"pass out\" but I just couldn't do it. I was so tired and confused and emotional and sad. I felt like I was having a breakdown. I was just so drained and scared. I just wanted to be by myself and I didn't want to do anything. \n",
      " I feel like I've learned so much. \n",
      " \n",
      "My fears are now well-known and that helps me feel less scared. \n",
      " \n",
      "After a couple of weeks, I started to see how easily we could pass out. \n",
      " \n",
      "There's nothing about my room that isn't neat and tidy. \n",
      " \n",
      "I can spend the whole night on my phone\n",
      " moved in was the worst. It was a mixture of sadness and frustration that I could never really be \"normal\". I mean, I had a job and friends and everything, but I had no idea what I wanted out of life. I was exhausted. I had no friends. I was depressed. I felt like I was worthless. But then, just the next day, we started having a party in the kitchen. It was just for me. I just sat at my desk and drank beer. \n",
      " We didn't really go out every day. I didn't go out to dinner or movies. But then, a few days later, we did a BBQ in the backyard and we ate there for the first time since moving in. It\n",
      "82\n",
      " \n",
      " The last three words, of course, were the truth. \n",
      " The X-Files\n",
      "The Unauthorized Guide to the Alien Universe\n",
      "By Michael Crichton, John W. Campbell, and John M. Ford\n",
      "\"I thought the X-Files was a show about aliens and was a mix of two genres: science fiction and fantasy. But, for some reason, it seemed to me that there was something deeper going on. It was a show that seemed to want to explore the psychology of the person who got abducted by aliens.\"\n",
      "-Mitch Pileggi\n",
      "\"I'd like to think that the X-Files was about science, but it's more about a very specific kind of science: psychology.\"\n",
      "\n",
      "\n",
      " A white light appeared from a large window and surrounded the praying mantis. \n",
      " The mantis' body was composed of a dense black gel. The gel was glistening like liquid metal, like the steel of a medieval warhammer. The praying mantis was more than a match for any creature in the room. The light from the window was brighter than the sun, and seemed to envelop the mantis completely. It was almost like a veil, and the light was as black as the praying mantis itself. The praying mantis, however, seemed to be in the deepest part of the blackness. It stared at Feruld, and for a second it seemed to look at the glass for the first time. Then, the\n",
      "8\n",
      " the air. It is nearly pitch black, the entire landscape is lit by hundreds of huge lights. I can barely make out the bodies of people walking down the street. Some of them are carrying the most obvious signs of injuries, some just barefoot, some have bags of grass under their arms.\n",
      "I try to get as close to one of the bodies as possible, the tats have no lines or shapes. It's like a person was made of tats. One of the homeless people stops and smiles, he looks so much like my brother.\n",
      "The rain has turned into a massive cloud, a towering pile of thunder. I don't have to look too hard to see the tats in the rain. I can't see the\n",
      " the mountains. The lights of the city are still bright as the sun begins to set but the dark is growing. The air is cold and wet and I feel like i am walking through the desert. The car whips around an intersection and I pull over, sit in it, and stare at the stars for a few minutes. Then the car drives away, leaving me alone and alone with my thoughts.\n",
      "In the morning I will not remember my life before that day. I will not be able to remember the joys of my life before the car pulled into the parking lot and the doors closed behind me. I will not be able to remember the joys of my life before the car pulled into the parking lot and the doors closed behind me\n",
      "15\n",
      " The ruins of the city. It was a city so vast, so old and so beautiful, that I saw my first human. There was a small stone cottage here, and I was looking around for the right place to start a conversation with the elderly woman who lives in the cottage. I looked into the cottage, and the man was sitting on a bench at the edge. \n",
      "I told him what I had seen, and he told me of a long-lost city. A long-lost city, which I now know to be an old abandoned city. The man was a good friend. He was a merchant, and he came with a caravan of merchants to trade. He was traveling alone, and I found him lying on the ground.\n",
      " I found the Lost City. The Lost City is an abandoned city that has been abandoned for thousands of years. The Lost City is where the secrets of humanity were discovered. \n",
      " The Lost City was inhabited by people who have left the mortal plane. \n",
      "I think we need to get out of here. \n",
      "The Lost City is very well preserved, so I believe it is a very good idea to find a way to get out of here. \n",
      "I think we need to get out of here. I found a way. I'm going to make a plan. \n",
      "I think we need to get out of here. \n",
      "We need to get out of here. We have found a way to get out of here. \n",
      "46\n",
      " on the same day, or on the same day and time. \n",
      " \n",
      " I am not saying this is necessarily what they do in every instance, but the simple fact that they do this as part of their mating ritual does lend itself to being called a mating ritual. The example I gave earlier was one of the more difficult ones, as it involved the separation of two of the other men. \n",
      " \n",
      " I'm sorry to be so frank. \n",
      " \n",
      " \n",
      "\n",
      " \n",
      " That's all for today. I'm off to bed. Until next time.<|endoftext|>According to sources, Aqib Talib is planning to test free agency after the 2016 season.\n",
      "\n",
      "He'll have plenty of suitors, as\n",
      " in a situation where both partners have a clear, but imperfect understanding of their respective reproductive strategy. \n",
      " \n",
      " In the next two stages we see a child asking the parent for permission to use a specific area of the house to play.  They are, in fact, asking to use the bathroom.  We can see how this ritual is a practice for establishing paternity.  We are reminded that it is the parent who has been asking, but it is the child who has the permission to use the bathroom.  This is a more complicated ritual, however, and not nearly as complicated as the one described above, though still fairly simple.  It is possible that the child was the only one who had this knowledge,\n",
      "47\n",
      ", but not shaken enough to quit \n",
      " \n",
      "\n",
      "But then I heard something moving on the other side of the lake  and I could not believe it  I had run straight into it. The grave digger. I had to have the body. But he would not tell me his name. So I called the police. I told them all I knew of the digger's identity. The name was Johnny Pemberton, and he worked for a company that excavated graves for the military. I also told them about the shadow on the lake. He said that he did not know what to do. I did not have a clue. I finally told the police I would just let them have the body if they let me\n",
      " but grateful \n",
      " That I'd been the one who found the bodies \n",
      " That I'd helped bring closure \n",
      " To a long-forgotten grave \n",
      " I buried the bodies \n",
      " I can still see the stone \n",
      "And it's my job to dig up the grave \n",
      "To finish what I started \n",
      "I've found the stone \n",
      " I buried the bodies \n",
      " I can still see the stone \n",
      "And it's my job to dig up the grave\n",
      " To finish what I started\n",
      "I've found the stone\n",
      " I buried the bodies\n",
      " I can still see the stone\n",
      "And it's my job to dig up the grave\n",
      " To finish what I started\n",
      " I\n"
     ]
    }
   ],
   "source": [
    "num_gens = 10\n",
    "file_name_base = 'test_big'\n",
    "answer_keys = []\n",
    "with open(file_name_base + '_blind_output.txt', 'w') as file: \n",
    "    \n",
    "    for ind in range(num_gens):\n",
    "    \n",
    "        select_a_random_prompt = np.random.choice(gs.shape[0])\n",
    "        select_random_generation_methods = np.random.choice(range(gs.shape[1]), size =gs.shape[1], replace=False )\n",
    "        #select_random_generation_methods = np.insert(select_random_generation_methods, 0, 0)\n",
    "        print(select_a_random_prompt)\n",
    "        select_random_generation_methods;\n",
    "        \n",
    "        answer_keys.append(gs.iloc[select_a_random_prompt,select_random_generation_methods].index.values)\n",
    "\n",
    "        file.write('Prompt: \\n \\n')\n",
    "        file.write(prompts_used[select_a_random_prompt])\n",
    "        file.write('\\n')\n",
    "        file.write('================== \\n')\n",
    "        for ind, out in enumerate(gs.iloc[select_a_random_prompt,select_random_generation_methods].tolist()):\n",
    "            file.write('Random Generation: '+str(ind)+' \\n \\n')\n",
    "            file.write(out)\n",
    "            print(out)\n",
    "\n",
    "            file.write('\\n')\n",
    "            file.write('================== \\n')\n",
    "            if ind == gs.shape[1]-1: # dont write random generation at the end. \n",
    "                continue\n",
    "                \n",
    "        file.write('\\n \\n')\n",
    "        file.write('=====================================================')\n",
    "        file.write('\\n \\n')\n",
    "        \n",
    "with open(file_name_base + '_answers.txt', 'w') as file: \n",
    "    for ak in answer_keys:\n",
    "        for ind, k in enumerate(ak):\n",
    "            file.write(str(ind)+' : '+k+' \\n')\n",
    "        file.write('\\n')\n",
    "        file.write('=====================================================')\n",
    "        file.write('\\n')\n",
    "pickle.dump(answer_keys,open(file_name_base + '_answers_list.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the very tight ones are too close and degenerate into repeats. The prompts and stochasticity of a single generation are really hard to deal with though. it is also a lot to read in order to evaluate the quality. \n",
    "What is something I can evaluate algorithmically and where diverse beam search is outperformed? It may inherently need to be something that is long. \n",
    "Seems like there is definitely a sweet spot where dont want too many options but also dont want it to be degenerate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Samp Type:n Param:0.69', 'Samp Type:tfs Param:0.95'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:n Param:0.69', 'Samp Type:tfs Param:0.95'], dtype=object),\n",
       " array(['Samp Type:n Param:0.69', 'Samp Type:tfs Param:0.95'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:n Param:0.69', 'Samp Type:tfs Param:0.95'], dtype=object)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing:\n",
    "\n",
    "\n",
    "alex_answers = [1, 0.5, 0.5, 0, 0, 0, 1, 0.5, 0, 1] # liked generation 0\n",
    "alex_chose = []\n",
    "\n",
    "for ind, a in enumerate(alex_answers):\n",
    "    if a==0.5:\n",
    "        continue\n",
    "    elif a==1:\n",
    "        want_key_ind = 0\n",
    "    elif a==0:\n",
    "        want_key_ind = 1\n",
    "    chosen = answer_keys[ind][want_key_ind]\n",
    "    alex_chose.append(chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Samp Type:n Param:0.69', 'Samp Type:tfs Param:0.95'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:n Param:0.69', 'Samp Type:tfs Param:0.95'], dtype=object),\n",
       " array(['Samp Type:n Param:0.69', 'Samp Type:tfs Param:0.95'], dtype=object),\n",
       " array(['Samp Type:tfs Param:0.95', 'Samp Type:n Param:0.69'], dtype=object),\n",
       " array(['Samp Type:n Param:0.69', 'Samp Type:tfs Param:0.95'], dtype=object)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Samp Type:n Param:0.69',\n",
       " 'Samp Type:n Param:0.69',\n",
       " 'Samp Type:n Param:0.69',\n",
       " 'Samp Type:n Param:0.69',\n",
       " 'Samp Type:n Param:0.69',\n",
       " 'Samp Type:n Param:0.69',\n",
       " 'Samp Type:n Param:0.69']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Samp Type:n Param:0.69', 'Samp Type:tfs Param:0.95'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.iloc[select_a_random_prompt,select_random_generation_methods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.iloc[select_a_random_prompt,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_prompt =rand_selects[select_a_random_prompt]\n",
    "choose_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompts.iloc[choose_prompt].test_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
