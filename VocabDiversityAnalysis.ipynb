{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Gram Analysis\n",
    "get every word generated by a method and its frequency. \n",
    "Get the 4grams of each of the words and their frequencies. Can then make these plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pandas as pd    \n",
    "import torch\n",
    "\n",
    "n_gram = 4\n",
    "prompt_length = 100\n",
    "num_batches = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to decode:\n",
    "from decodeLogits import *\n",
    "vals_dict = {'tfs':[0.25, 0.75, 0.9, 0.95, 0.99], 'flat':[0.01, 0.02, 0.05],\n",
    "'n': [0.1, 0.25, 0.5, 0.75, 0.9], 'k':[1,10,40,200]  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key is: tfs\n",
      "opening file: gpt-2_output/all_logits_tfs-sampling-type_0.25-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz\n"
     ]
    }
   ],
   "source": [
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        if par ==None:\n",
    "            par = \"None\"\n",
    "        print('opening file:', 'gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz')\n",
    "        #all_logits = pickle.load( gzip.open('gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        text = pickle.load( gzip.open('gpt-2_output/all_text_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        break\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 250)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of samples: 100\n"
     ]
    }
   ],
   "source": [
    "tot_num = 0\n",
    "for i in range(num_batches):\n",
    "    tot_num+= text[i].shape[0]\n",
    "print('total number of samples:', tot_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# works for all but the generated text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tot_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4e75539f371f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtot_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tot_num' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pandas as pd    \n",
    "import torch\n",
    "\n",
    "num_samples = tot_num\n",
    "num_batches = num_samples//batch_size\n",
    "\n",
    "word_frequency_for_all = dict()\n",
    "\n",
    "num_target_too_short = 0\n",
    "\n",
    "for key, params in vals_dict.items():\n",
    "    print('Key is:', key)\n",
    "    for par in params:\n",
    "        if par ==None:\n",
    "            par = \"None\"\n",
    "        print('opening file:', 'gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz')\n",
    "        #all_logits = pickle.load( gzip.open('gpt-2_output/all_logits_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        text = pickle.load( gzip.open('gpt-2_output/all_text_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        #rand_selects = pickle.load( gzip.open('gpt-2_output/prompt_rand_selections_'+key+'-sampling-type_'+str(par)+'-sampling-param_100-word-prompts_150-gen-length_100-number-of-prompts.pickle.gz', 'rb'))\n",
    "        #temp_tail_cdfs = np.zeros([num_samples, all_logits[0].shape[2]])\n",
    "        #temp_tail_ids = np.zeros([num_samples, all_logits[0].shape[2]])\n",
    "        p_ind = 0\n",
    "        word_freq_temp = dict()\n",
    "        for batch in range(num_batches):\n",
    "            for ind in range(0,text[0].shape[0]):\n",
    "                \n",
    "                if p_ind%25 == 0:\n",
    "                    print('index', str(p_ind))\n",
    "                    \n",
    "                # this is the ground truth calculations =====\n",
    "\n",
    "                decoded_text_generated = decoder_text(text[batch][ind, prompt_length:])\n",
    "                \n",
    "                word_set = list(set(decoded_text_generated.split(' '))) # may be some fuction that does this\n",
    "                for w in word_set:\n",
    "                    # count the number of occurences.\n",
    "                    pass\n",
    "                    #word_freq_temp[]\n",
    "                \n",
    "                word_frequency_for_all[key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)] = ground_token_in_cut_temp\n",
    "                \n",
    "                p_ind+=1\n",
    "                \n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key+'-sampling-type_'+str(par)+'prompt_'+str(p_ind)]\n",
    "\n",
    "for key, prompt_dicts in word_frequency_for_all.items():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
